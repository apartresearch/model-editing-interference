{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["DcZG9rm2IAiA","OLkInsdjyHMx","DQF0lzuokQer","6aByCOwUxQm7","o56Altbhz7i6"],"gpuType":"T4","authorship_tag":"ABX9TyNqJSwnJXCa0zuTq9YNdaFw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"b13177b7"},"source":["<a href=\"https://colab.research.google.com/github/wlg100/numseqcont_circuit_expms/blob/main/notebook_templates/minimal_circuit_template.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"left\"/></a>&nbsp;or in a local notebook."]},{"cell_type":"markdown","metadata":{"id":"DcZG9rm2IAiA"},"source":["# Setup\n","(No need to change anything)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rMcpSDdjIAiA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"014060ef-75f6-43d0-ec57-2f8792aa81e8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running as a Colab notebook\n","Collecting git+https://github.com/neelnanda-io/TransformerLens.git\n","  Cloning https://github.com/neelnanda-io/TransformerLens.git to /tmp/pip-req-build-o3tsl1as\n","  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/TransformerLens.git /tmp/pip-req-build-o3tsl1as\n","  Resolved https://github.com/neelnanda-io/TransformerLens.git to commit fa287750606075574df2c538058e67d648e2f952\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting accelerate>=0.23.0 (from transformer-lens==0.0.0)\n","  Downloading accelerate-0.24.0-py3-none-any.whl (260 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting beartype<0.15.0,>=0.14.1 (from transformer-lens==0.0.0)\n","  Downloading beartype-0.14.1-py3-none-any.whl (739 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets>=2.7.1 (from transformer-lens==0.0.0)\n","  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting einops>=0.6.0 (from transformer-lens==0.0.0)\n","  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fancy-einsum>=0.0.3 (from transformer-lens==0.0.0)\n","  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n","Collecting jaxtyping>=0.2.11 (from transformer-lens==0.0.0)\n","  Downloading jaxtyping-0.2.23-py3-none-any.whl (29 kB)\n","Collecting numpy>=1.24 (from transformer-lens==0.0.0)\n","  Downloading numpy-1.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu12>=12.1.3.1 (from transformer-lens==0.0.0)\n","  Downloading nvidia_cublas_cu12-12.3.2.9-py3-none-manylinux1_x86_64.whl (417.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu12>=12.1.105 (from transformer-lens==0.0.0)\n","  Downloading nvidia_cuda_cupti_cu12-12.3.52-py3-none-manylinux1_x86_64.whl (14.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12>=12.1.105 (from transformer-lens==0.0.0)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.3.52-py3-none-manylinux1_x86_64.whl (24.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.9/24.9 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu12>=12.1.105 (from transformer-lens==0.0.0)\n","  Downloading nvidia_cuda_runtime_cu12-12.3.52-py3-none-manylinux1_x86_64.whl (867 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.7/867.7 kB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu12>=8.9.2.26 (from transformer-lens==0.0.0)\n","  Downloading nvidia_cudnn_cu12-8.9.4.25-py3-none-manylinux1_x86_64.whl (720.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m720.1/720.1 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu12>=11.0.2.54 (from transformer-lens==0.0.0)\n","  Downloading nvidia_cufft_cu12-11.0.11.19-py3-none-manylinux1_x86_64.whl (98.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.8/98.8 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu12>=10.3.2.106 (from transformer-lens==0.0.0)\n","  Downloading nvidia_curand_cu12-10.3.4.52-py3-none-manylinux1_x86_64.whl (56.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu12>=11.4.5.107 (from transformer-lens==0.0.0)\n","  Downloading nvidia_cusolver_cu12-11.5.3.52-py3-none-manylinux1_x86_64.whl (125.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu12>=12.1.0.106 (from transformer-lens==0.0.0)\n","  Downloading nvidia_cusparse_cu12-12.1.3.153-py3-none-manylinux1_x86_64.whl (195.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.6/195.6 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu12>=2.18.1 (from transformer-lens==0.0.0)\n","  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu12>=12.1.105 (from transformer-lens==0.0.0)\n","  Downloading nvidia_nvtx_cu12-12.3.52-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (1.5.3)\n","Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (13.6.0)\n","Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (2.1.0+cu118)\n","Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (4.66.1)\n","Collecting transformers>=4.25.1 (from transformer-lens==0.0.0)\n","  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m118.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: triton>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (2.1.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (4.5.0)\n","Collecting wandb>=0.13.5 (from transformer-lens==0.0.0)\n","  Downloading wandb-0.15.12-py3-none-any.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (6.0.1)\n","Collecting huggingface-hub (from accelerate>=0.23.0->transformer-lens==0.0.0)\n","  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (9.0.0)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets>=2.7.1->transformer-lens==0.0.0)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (2.31.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (3.4.1)\n","Collecting multiprocess (from datasets>=2.7.1->transformer-lens==0.0.0)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (3.8.6)\n","Collecting typeguard<3,>=2.13.3 (from jaxtyping>=0.2.11->transformer-lens==0.0.0)\n","  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12>=11.4.5.107->transformer-lens==0.0.0)\n","  Downloading nvidia_nvjitlink_cu12-12.3.52-py3-none-manylinux1_x86_64.whl (20.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2023.3.post1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer-lens==0.0.0) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer-lens==0.0.0) (2.16.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (3.12.4)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (3.1.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->transformer-lens==0.0.0) (2023.6.3)\n","Collecting tokenizers<0.15,>=0.14 (from transformers>=4.25.1->transformer-lens==0.0.0)\n","  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers>=4.25.1->transformer-lens==0.0.0)\n","  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (8.1.7)\n","Collecting GitPython!=3.1.29,>=1.0.0 (from wandb>=0.13.5->transformer-lens==0.0.0)\n","  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentry-sdk>=1.0.0 (from wandb>=0.13.5->transformer-lens==0.0.0)\n","  Downloading sentry_sdk-1.32.0-py2.py3-none-any.whl (240 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.0/241.0 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb>=0.13.5->transformer-lens==0.0.0)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting pathtools (from wandb>=0.13.5->transformer-lens==0.0.0)\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting setproctitle (from wandb>=0.13.5->transformer-lens==0.0.0)\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (67.7.2)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (1.4.4)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (3.20.3)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer-lens==0.0.0) (1.16.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (3.3.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.3.1)\n","Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens==0.0.0)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer-lens==0.0.0) (0.1.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (2023.7.22)\n","Collecting huggingface-hub (from accelerate>=0.23.0->transformer-lens==0.0.0)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->transformer-lens==0.0.0) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->transformer-lens==0.0.0) (1.3.0)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens==0.0.0)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Building wheels for collected packages: transformer-lens, pathtools\n","  Building wheel for transformer-lens (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformer-lens: filename=transformer_lens-0.0.0-py3-none-any.whl size=116432 sha256=c797ec6c169bd4dd3b94c94a6a6e6c77438ff526c16d7937b5c638131c77e4db\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-_oiiux69/wheels/8a/1e/37/ffb9c15454a1725b13a9d9f5e74fb91725048884ad734b8c1f\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=f9aa9e5f24a21bde841ec2dd20851a459112d24dfb0a0de207e28706c63272e4\n","  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n","Successfully built transformer-lens pathtools\n","Installing collected packages: pathtools, typeguard, smmap, setproctitle, sentry-sdk, safetensors, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, fancy-einsum, einops, docker-pycreds, dill, beartype, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, jaxtyping, huggingface-hub, gitdb, tokenizers, nvidia-cusolver-cu12, GitPython, accelerate, wandb, transformers, datasets, transformer-lens\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.23.5\n","    Uninstalling numpy-1.23.5:\n","      Successfully uninstalled numpy-1.23.5\n"]}],"source":["# Janky code to do different setup when run in a Colab notebook vs VSCode\n","DEBUG_MODE = False\n","try:\n","    import google.colab\n","    IN_COLAB = True\n","    print(\"Running as a Colab notebook\")\n","    %pip install git+https://github.com/neelnanda-io/TransformerLens.git\n","    # Install another version of node that makes PySvelte work way faster\n","    !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n","    %pip install git+https://github.com/neelnanda-io/PySvelte.git\n","except:\n","    IN_COLAB = False\n","    print(\"Running as a Jupyter notebook - intended for development only!\")\n","    from IPython import get_ipython\n","\n","    ipython = get_ipython()\n","    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n","    ipython.magic(\"load_ext autoreload\")\n","    ipython.magic(\"autoreload 2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xKoTs7VBIAiD"},"outputs":[],"source":["# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n","import plotly.io as pio\n","\n","if IN_COLAB or not DEBUG_MODE:\n","    # Thanks to annoying rendering issues, Plotly graphics will either show up in colab OR Vscode depending on the renderer - this is bad for developing demos! Thus creating a debug mode.\n","    pio.renderers.default = \"colab\"\n","else:\n","    pio.renderers.default = \"png\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z6b1n2tvIAiD"},"outputs":[],"source":["# Import stuff\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import einops\n","from fancy_einsum import einsum\n","import tqdm.notebook as tqdm\n","import random\n","from pathlib import Path\n","import plotly.express as px\n","from torch.utils.data import DataLoader\n","\n","from jaxtyping import Float, Int\n","from typing import List, Union, Optional\n","from functools import partial\n","import copy\n","\n","import itertools\n","from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n","import dataclasses\n","import datasets\n","from IPython.display import HTML"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zuhzYxbsIAiE"},"outputs":[],"source":["# import pysvelte\n","\n","import transformer_lens\n","import transformer_lens.utils as utils\n","from transformer_lens.hook_points import (\n","    HookedRootModule,\n","    HookPoint,\n",")  # Hooking utilities\n","from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"]},{"cell_type":"markdown","metadata":{"id":"hccba0v-IAiF"},"source":["We turn automatic differentiation off, to save GPU memory, as this notebook focuses on model inference not model training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cFMTUcQiIAiF"},"outputs":[],"source":["torch.set_grad_enabled(False)"]},{"cell_type":"markdown","metadata":{"id":"zyKb4C51IAiG"},"source":["Plotting helper functions:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KFs9BrbzIAiH"},"outputs":[],"source":["def imshow(tensor, renderer=None, **kwargs):\n","    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", **kwargs).show(renderer)\n","\n","def line(tensor, renderer=None, **kwargs):\n","    px.line(y=utils.to_numpy(tensor), **kwargs).show(renderer)\n","\n","def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n","    x = utils.to_numpy(x)\n","    y = utils.to_numpy(y)\n","    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"]},{"cell_type":"markdown","source":["## Load Model"],"metadata":{"id":"OLkInsdjyHMx"}},{"cell_type":"markdown","source":["Decide which model to use (eg. gpt2-small vs -medium)"],"metadata":{"id":"ssJgoKr2yI8O"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xLwDyosvIAiJ"},"outputs":[],"source":["model = HookedTransformer.from_pretrained(\n","    \"gpt2-small\",\n","    center_unembed=True,\n","    center_writing_weights=True,\n","    fold_ln=True,\n","    refactor_factored_attn_matrices=True,\n",")"]},{"cell_type":"markdown","source":["## Import functions from repo"],"metadata":{"id":"Z4iJEGh6b56v"}},{"cell_type":"code","source":["!git clone https://github.com/callummcdougall/ARENA_2.0.git"],"metadata":{"id":"Fdh5--MfYw7-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cd ARENA_2.0/chapter1_transformers/exercises/part3_indirect_object_identification"],"metadata":{"id":"iZ4C_bsXZFfj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import ioi_circuit_extraction as ioi_circuit_extraction"],"metadata":{"id":"OT0Sn571ZnkV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Generate dataset with multiple prompts"],"metadata":{"id":"cGX9iHAz_UKX"}},{"cell_type":"markdown","source":["### test prompts"],"metadata":{"id":"DQF0lzuokQer"}},{"cell_type":"code","source":["modeltest = HookedTransformer.from_pretrained(\"gpt2-small\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FW7ZRS0wtqs4","executionInfo":{"status":"ok","timestamp":1698588583907,"user_tz":240,"elapsed":11963,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"318c5bcf-71dc-42c4-d051-ae4f64112871"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded pretrained model gpt2-small into HookedTransformer\n"]}]},{"cell_type":"code","source":["example_prompt = \"The war lasted from the year 1750 to the year 17\"\n","example_answer = \" 51\"\n","utils.test_prompt(example_prompt, example_answer, modeltest, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":275},"id":"OQpqlEjwED7b","executionInfo":{"status":"ok","timestamp":1698588637916,"user_tz":240,"elapsed":451,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"bb2070a6-f0c3-4e25-ce0c-ba83599418ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'The', ' war', ' lasted', ' from', ' the', ' year', ' 17', '50', ' to', ' the', ' year', ' 17']\n","Tokenized answer: [' 51']\n"]},{"output_type":"display_data","data":{"text/plain":["Performance on answer token:\n","\u001b[1mRank: \u001b[0m\u001b[1;36m1096\u001b[0m\u001b[1m     Logit: \u001b[0m\u001b[1;36m11.83\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.00\u001b[0m\u001b[1m% Token: | \u001b[0m\u001b[1;36m51\u001b[0m\u001b[1m|\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n","<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1096</span><span style=\"font-weight: bold\">     Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11.83</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span><span style=\"font-weight: bold\">% Token: | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">51</span><span style=\"font-weight: bold\">|</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Top 0th token. Logit: 30.08 Prob: 26.90% Token: |60|\n","Top 1th token. Logit: 29.10 Prob: 10.12% Token: |75|\n","Top 2th token. Logit: 29.02 Prob:  9.33% Token: |70|\n","Top 3th token. Logit: 28.62 Prob:  6.29% Token: |90|\n","Top 4th token. Logit: 28.43 Prob:  5.19% Token: |80|\n","Top 5th token. Logit: 28.28 Prob:  4.45% Token: |50|\n","Top 6th token. Logit: 27.82 Prob:  2.83% Token: |55|\n","Top 7th token. Logit: 27.41 Prob:  1.87% Token: |65|\n","Top 8th token. Logit: 27.34 Prob:  1.75% Token: |76|\n","Top 9th token. Logit: 27.17 Prob:  1.47% Token: |71|\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' 51'\u001b[0m, \u001b[1;36m1096\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' 51'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1096</span><span style=\"font-weight: bold\">)]</span>\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["example_prompt = \"The war lasted from the year 1701 to the year 17\"\n","example_answer = \" 51\"\n","utils.test_prompt(example_prompt, example_answer, modeltest, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":275},"id":"mkYf2ymhxc2o","executionInfo":{"status":"ok","timestamp":1698588650796,"user_tz":240,"elapsed":358,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"1b465a84-72d5-4dcd-a73d-6b12ba15b766"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'The', ' war', ' lasted', ' from', ' the', ' year', ' 17', '01', ' to', ' the', ' year', ' 17']\n","Tokenized answer: [' 51']\n"]},{"output_type":"display_data","data":{"text/plain":["Performance on answer token:\n","\u001b[1mRank: \u001b[0m\u001b[1;36m1548\u001b[0m\u001b[1m     Logit:  \u001b[0m\u001b[1;36m9.81\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.00\u001b[0m\u001b[1m% Token: | \u001b[0m\u001b[1;36m51\u001b[0m\u001b[1m|\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n","<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1548</span><span style=\"font-weight: bold\">     Logit:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.81</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span><span style=\"font-weight: bold\">% Token: | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">51</span><span style=\"font-weight: bold\">|</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Top 0th token. Logit: 26.03 Prob:  5.33% Token: |20|\n","Top 1th token. Logit: 26.00 Prob:  5.19% Token: |15|\n","Top 2th token. Logit: 25.89 Prob:  4.62% Token: |02|\n","Top 3th token. Logit: 25.85 Prob:  4.45% Token: |12|\n","Top 4th token. Logit: 25.75 Prob:  4.02% Token: |10|\n","Top 5th token. Logit: 25.64 Prob:  3.62% Token: |18|\n","Top 6th token. Logit: 25.58 Prob:  3.38% Token: |05|\n","Top 7th token. Logit: 25.56 Prob:  3.32% Token: |03|\n","Top 8th token. Logit: 25.53 Prob:  3.23% Token: |16|\n","Top 9th token. Logit: 25.52 Prob:  3.20% Token: |04|\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' 51'\u001b[0m, \u001b[1;36m1548\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' 51'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1548</span><span style=\"font-weight: bold\">)]</span>\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"markdown","source":["#### less-than using in-context"],"metadata":{"id":"9Sk0G61l4Vlh"}},{"cell_type":"markdown","source":["See pg22 of greater-than paper: we address the tasks “The <noun> ended in the year 17YY\n","and started in the year 17” and “The <noun> lasted from the year 7YY BC to the year 7”, which do\n","use our circuit, but should not do so.\n","\n","These complete it with 'greater-than'"],"metadata":{"id":"4cyG3zqJ4mSX"}},{"cell_type":"code","source":["example_prompt = \"The war ended in the year 1750 and started in the year 17\"\n","example_answer = \" 49\"\n","utils.test_prompt(example_prompt, example_answer, modeltest, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":275},"id":"oF4OEHDt4W0l","executionInfo":{"status":"ok","timestamp":1698590507696,"user_tz":240,"elapsed":1245,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"e5c55557-b915-40af-e3a7-b65988843e4a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'The', ' war', ' ended', ' in', ' the', ' year', ' 17', '50', ' and', ' started', ' in', ' the', ' year', ' 17']\n","Tokenized answer: [' 49']\n"]},{"output_type":"display_data","data":{"text/plain":["Performance on answer token:\n","\u001b[1mRank: \u001b[0m\u001b[1;36m1134\u001b[0m\u001b[1m     Logit: \u001b[0m\u001b[1;36m10.12\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.00\u001b[0m\u001b[1m% Token: | \u001b[0m\u001b[1;36m49\u001b[0m\u001b[1m|\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n","<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1134</span><span style=\"font-weight: bold\">     Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10.12</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span><span style=\"font-weight: bold\">% Token: | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">49</span><span style=\"font-weight: bold\">|</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Top 0th token. Logit: 26.42 Prob: 18.75% Token: |60|\n","Top 1th token. Logit: 25.20 Prob:  5.55% Token: |50|\n","Top 2th token. Logit: 25.19 Prob:  5.51% Token: |61|\n","Top 3th token. Logit: 25.03 Prob:  4.66% Token: |51|\n","Top 4th token. Logit: 24.97 Prob:  4.38% Token: |55|\n","Top 5th token. Logit: 24.74 Prob:  3.51% Token: |52|\n","Top 6th token. Logit: 24.68 Prob:  3.30% Token: |75|\n","Top 7th token. Logit: 24.62 Prob:  3.10% Token: |59|\n","Top 8th token. Logit: 24.62 Prob:  3.10% Token: |70|\n","Top 9th token. Logit: 24.60 Prob:  3.05% Token: |56|\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' 49'\u001b[0m, \u001b[1;36m1134\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' 49'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1134</span><span style=\"font-weight: bold\">)]</span>\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["example_prompt = \"The war ended in the year 1790 and started in the year 1780. The war ended in the year 1750 and started in the year 17\"\n","example_answer = \" 49\"\n","utils.test_prompt(example_prompt, example_answer, modeltest, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"id":"tv_bcxQL4rvC","executionInfo":{"status":"ok","timestamp":1698590563898,"user_tz":240,"elapsed":16,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"8eeb3290-410a-499f-bc39-aaaf579451e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'The', ' war', ' ended', ' in', ' the', ' year', ' 17', '90', ' and', ' started', ' in', ' the', ' year', ' 17', '80', '.', ' The', ' war', ' ended', ' in', ' the', ' year', ' 17', '50', ' and', ' started', ' in', ' the', ' year', ' 17']\n","Tokenized answer: [' 49']\n"]},{"output_type":"display_data","data":{"text/plain":["Performance on answer token:\n","\u001b[1mRank: \u001b[0m\u001b[1;36m1161\u001b[0m\u001b[1m     Logit: \u001b[0m\u001b[1;36m10.34\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.00\u001b[0m\u001b[1m% Token: | \u001b[0m\u001b[1;36m49\u001b[0m\u001b[1m|\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n","<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1161</span><span style=\"font-weight: bold\">     Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10.34</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span><span style=\"font-weight: bold\">% Token: | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">49</span><span style=\"font-weight: bold\">|</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Top 0th token. Logit: 26.13 Prob: 16.52% Token: |80|\n","Top 1th token. Logit: 25.79 Prob: 11.82% Token: |60|\n","Top 2th token. Logit: 25.79 Prob: 11.77% Token: |70|\n","Top 3th token. Logit: 25.65 Prob: 10.25% Token: |90|\n","Top 4th token. Logit: 25.36 Prob:  7.65% Token: |50|\n","Top 5th token. Logit: 24.92 Prob:  4.94% Token: |75|\n","Top 6th token. Logit: 24.43 Prob:  3.01% Token: |85|\n","Top 7th token. Logit: 24.25 Prob:  2.53% Token: |40|\n","Top 8th token. Logit: 24.04 Prob:  2.04% Token: |55|\n","Top 9th token. Logit: 24.00 Prob:  1.97% Token: |65|\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' 49'\u001b[0m, \u001b[1;36m1161\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' 49'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1161</span><span style=\"font-weight: bold\">)]</span>\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["example_prompt = \"90 is less than 100. 80 is less than 90. 70 is less than\"\n","example_answer = \" 80\"\n","utils.test_prompt(example_prompt, example_answer, modeltest, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":275},"id":"t1wIMQE94zRM","executionInfo":{"status":"ok","timestamp":1698590600848,"user_tz":240,"elapsed":984,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"47f7ec05-e1b7-425f-a7ef-c904dc106511"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', '90', ' is', ' less', ' than', ' 100', '.', ' 80', ' is', ' less', ' than', ' 90', '.', ' 70', ' is', ' less', ' than']\n","Tokenized answer: [' 80']\n"]},{"output_type":"display_data","data":{"text/plain":["Performance on answer token:\n","\u001b[1mRank: \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m17.60\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m1.15\u001b[0m\u001b[1m% Token: | \u001b[0m\u001b[1;36m80\u001b[0m\u001b[1m|\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n","<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17.60</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.15</span><span style=\"font-weight: bold\">% Token: | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span><span style=\"font-weight: bold\">|</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Top 0th token. Logit: 22.02 Prob: 95.21% Token: | 70|\n","Top 1th token. Logit: 17.60 Prob:  1.15% Token: | 90|\n","Top 2th token. Logit: 17.60 Prob:  1.15% Token: | 80|\n","Top 3th token. Logit: 16.81 Prob:  0.52% Token: | 75|\n","Top 4th token. Logit: 16.29 Prob:  0.31% Token: | 20|\n","Top 5th token. Logit: 16.06 Prob:  0.25% Token: | 65|\n","Top 6th token. Logit: 15.80 Prob:  0.19% Token: | 50|\n","Top 7th token. Logit: 15.63 Prob:  0.16% Token: | 60|\n","Top 8th token. Logit: 15.00 Prob:  0.08% Token: | 40|\n","Top 9th token. Logit: 14.66 Prob:  0.06% Token: | 85|\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' 80'\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' 80'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)]</span>\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"markdown","source":["## test tokenizer to make pos_dict, prompt_dict"],"metadata":{"id":"IK_8tj2_xN3B"}},{"cell_type":"code","source":["model.tokenizer('1701')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5FgAMmablGUD","executionInfo":{"status":"ok","timestamp":1698585421600,"user_tz":240,"elapsed":445,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"1f8c3353-86ed-455e-e9b9-f8c154bb9dce"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [1558, 486], 'attention_mask': [1, 1]}"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["model.tokenizer('01')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OpQPaWj8lLH5","executionInfo":{"status":"ok","timestamp":1698585429618,"user_tz":240,"elapsed":419,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"48e0a15c-0c17-43ac-b639-fcdaa5b3c12e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [486], 'attention_mask': [1]}"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["model.tokenizer.convert_tokens_to_string(model.tokenizer.convert_ids_to_tokens(model.tokenizer('1701')['input_ids']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"dVtPScHGlz0u","executionInfo":{"status":"ok","timestamp":1698585638519,"user_tz":240,"elapsed":457,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"436785e1-555f-456d-9154-011106b4d7e2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1701'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["# model.tokenizer.decode(486)\n","model.tokenizer.decode([486])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"C_q7hfw-mE9_","executionInfo":{"status":"ok","timestamp":1698586354335,"user_tz":240,"elapsed":14,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"738f6dd3-5258-494b-a2f0-81fc0a2e29fa"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'01'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["len(model.tokenizer()['input_ids'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MsXbxrlhlWM1","executionInfo":{"status":"ok","timestamp":1698585504932,"user_tz":240,"elapsed":6,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"9f3f9736-7566-4b5a-8ee8-0644f579ee0c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["11"]},"metadata":{},"execution_count":35}]},{"cell_type":"markdown","source":["### get rid of \"Ġ\" +"],"metadata":{"id":"5FnwoB_OwDPu"}},{"cell_type":"code","source":["model.tokenizer(\"1711\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XDIf7a8Dvlm_","executionInfo":{"status":"ok","timestamp":1696390342567,"user_tz":240,"elapsed":30,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"6c025c84-148f-4d70-f208-6207d95054e5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [1558, 1157], 'attention_mask': [1, 1]}"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["model.tokenizer.tokenize(\"1711\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EztMlBtpvEON","executionInfo":{"status":"ok","timestamp":1696390332388,"user_tz":240,"elapsed":322,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"4258ecaa-8ea2-4267-8749-09ab848f299a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['17', '11']"]},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","source":["B/c 11 is after 17 and doesn't have a space in front, get rid of \"Ġ\" +"],"metadata":{"id":"fX-WO3z2vy0u"}},{"cell_type":"code","source":["def get_prompts_pos_dicts(input_text, YY):\n","    pos_dict = {}\n","    prompt_dict = {}\n","    tokens_list = model.tokenizer(input_text)['input_ids']\n","\n","    for index, token in enumerate(tokens_list):\n","        token_as_string = model.tokenizer.decode(token)\n","        # if token_as_string == YY:\n","        #     key = 'YY'\n","        # else:\n","        #     key = 'T'+str(index)\n","        key = 'T'+str(index)\n","        pos_dict[key] = index\n","        prompt_dict[key] = token_as_string\n","    prompt_dict['text'] = input_text\n","\n","    return pos_dict, prompt_dict"],"metadata":{"id":"DLmqcJXklmWV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_text = 'The war lasted from the year 1750 to the year 17'\n","get_prompts_pos_dicts(input_text, '50')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jJMt8azbpNaA","executionInfo":{"status":"ok","timestamp":1698590848446,"user_tz":240,"elapsed":347,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"da36537f-dcc8-49dc-99e6-ad0a472cdb79"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["({'T0': 0,\n","  'T1': 1,\n","  'T2': 2,\n","  'T3': 3,\n","  'T4': 4,\n","  'T5': 5,\n","  'T6': 6,\n","  'T7': 7,\n","  'T8': 8,\n","  'T9': 9,\n","  'T10': 10,\n","  'T11': 11},\n"," {'T0': 'The',\n","  'T1': ' war',\n","  'T2': ' lasted',\n","  'T3': ' from',\n","  'T4': ' the',\n","  'T5': ' year',\n","  'T6': ' 17',\n","  'T7': '50',\n","  'T8': ' to',\n","  'T9': ' the',\n","  'T10': ' year',\n","  'T11': ' 17',\n","  'text': 'The war lasted from the year 1750 to the year 17'})"]},"metadata":{},"execution_count":186}]},{"cell_type":"markdown","source":["## make datasets"],"metadata":{"id":"6aByCOwUxQm7"}},{"cell_type":"code","source":["class Dataset:\n","    def __init__(self, prompts, pos_dict, tokenizer, S1_is_first=False):\n","        self.prompts = prompts\n","        self.tokenizer = tokenizer\n","        self.N = len(prompts)\n","        self.max_len = max(\n","            [\n","                len(self.tokenizer(prompt[\"text\"]).input_ids)\n","                for prompt in self.prompts\n","            ]\n","        )\n","        # all_ids = [prompt[\"TEMPLATE_IDX\"] for prompt in self.ioi_prompts]\n","        all_ids = [0 for prompt in self.prompts] # only 1 template\n","        all_ids_ar = np.array(all_ids)\n","        self.groups = []\n","        for id in list(set(all_ids)):\n","            self.groups.append(np.where(all_ids_ar == id)[0])\n","\n","        texts = [ prompt[\"text\"] for prompt in self.prompts ]\n","        self.toks = torch.Tensor(self.tokenizer(texts, padding=True).input_ids).type(\n","            torch.int\n","        )\n","        # self.io_tokenIDs = [\n","        #     self.tokenizer.encode(\" \" + prompt[\"corr\"])[0] for prompt in self.prompts\n","        # ]\n","        # self.s_tokenIDs = [\n","        #     self.tokenizer.encode(\" \" + prompt[\"incorr\"])[0] for prompt in self.prompts\n","        # ]\n","\n","        # self.YY = int()\n","\n","        # word_idx: for every prompt, find the token index of each target token and \"end\"\n","        # word_idx is a tensor with an element for each prompt. The element is the targ token's ind at that prompt\n","        self.word_idx = {}\n","        for targ in [key for key in self.prompts[0].keys() if (key != 'text' and key != 'corr' and key != 'incorr')]:\n","            targ_lst = []\n","            for prompt in self.prompts:\n","                input_text = prompt[\"text\"]\n","                tokens = model.tokenizer.tokenize(input_text)\n","                # if S1_is_first and targ == \"S1\":  # only use this if first token doesn't have space Ġ in front\n","                #     target_token = prompt[targ]\n","                # else:\n","                #     target_token = \"Ġ\" + prompt[targ]\n","                # target_index = tokens.index(target_token)\n","                target_index = pos_dict[targ]\n","                targ_lst.append(target_index)\n","            self.word_idx[targ] = torch.tensor(targ_lst)\n","\n","        targ_lst = []\n","        for prompt in self.prompts:\n","            input_text = prompt[\"text\"]\n","            tokens = self.tokenizer.tokenize(input_text)\n","            end_token_index = len(tokens) - 1\n","            targ_lst.append(end_token_index)\n","        self.word_idx[\"end\"] = torch.tensor(targ_lst)\n","\n","    def __len__(self):\n","        return self.N"],"metadata":{"id":"4wXBNWj5FwVn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_prompts_list(x, y):\n","    prompts_list = []\n","    for YY in range(x, y):\n","        input_text = f'The war lasted from the year 17{YY} to the year 17'\n","        pos_dict, prompt_dict = get_prompts_pos_dicts(input_text, YY)\n","        prompts_list.append(prompt_dict)\n","    return pos_dict, prompts_list\n","\n","# prompts_list = generate_prompts_list(45, 55)\n","pos_dict, prompts_list = generate_prompts_list(50, 51)\n","# prompts_list\n","dataset = Dataset(prompts_list, pos_dict, model.tokenizer, S1_is_first=True)"],"metadata":{"id":"u0NPSKcZ1iDe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_prompts_list_corr(x, y):\n","    prompts_list = []\n","    # for YY in range(x, y):\n","    YY = '01'\n","    input_text = f'The war lasted from the year 17{YY} to the year 17'\n","    pos_dict, prompt_dict = get_prompts_pos_dicts(input_text, YY)\n","    prompts_list.append(prompt_dict)\n","    return pos_dict, prompts_list\n","\n","# prompts_list = generate_prompts_list(45, 55)\n","pos_dict, prompts_list_2 = generate_prompts_list_corr(50, 51)\n","# prompts_list_2\n","dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer, S1_is_first=True)"],"metadata":{"id":"ViiKAFwBvmEG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## obtain the logits of each number between YY and 99"],"metadata":{"id":"A0W-GaM6Vfm-"}},{"cell_type":"code","source":["logits = torch.randn(32, 100, 256)  # [batch size, seq len, vocab size]\n","logits[range(logits.size(0)), [99]*logits.size(0), [5]*logits.size(0)] == logits[range(logits.size(0)), 99, 5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8y8oMWtPfyfj","executionInfo":{"status":"ok","timestamp":1698584676515,"user_tz":240,"elapsed":10,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"4f357603-f221-431d-a4e2-2fd9d774fa33"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True, True, True, True, True,\n","        True, True, True, True, True, True, True, True])"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["logits[range(logits.size(0)), 99, 5].size() # logits[range(logits.size(0)), dataset.word_idx[\"end\"], dataset.io_tokenIDs]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"90HU1KXxiXFu","executionInfo":{"status":"ok","timestamp":1698584689662,"user_tz":240,"elapsed":8,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"f6127e1f-8d13-42d8-cb03-a6ff9ffc8814"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([32])"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["# obtain the logits of each number between YY and 99, where YY is a two digit integer\n","\n","import torch\n","\n","def get_logits_for_range(logits, start_num, end_num, vocab):\n","    \"\"\"\n","    :param tensor: The logits tensor with dimensions [batch size, seq len, vocab size]\n","    :param start_num: The starting number\n","    :param end_num: The ending number\n","    :param vocab: A list or dictionary mapping of the vocabulary\n","    :return: A tensor containing logits for numbers between start_num and end_num\n","    \"\"\"\n","    # Getting indices for numbers between start_num and end_num\n","    # indices = [vocab[str(num)] for num in range(start_num, end_num+1)]\n","    indices = []\n","    for num in range(start_num, end_num+1):\n","        num_as_vocabID = model.tokenizer(str(num))['input_ids'][0]\n","        indices.append(num_as_vocabID)\n","\n","    # Extract logits for these indices\n","    logits_for_range = logits[:, logits.size(1)-1, indices]\n","\n","    return logits_for_range\n","\n","# Example usage:\n","tensor = torch.randn(32, 100, 50000)  # [batch size, seq len, vocab size]\n","vocab = {str(i): i for i in range(50000)}  # Example vocab mapping\n","YY = 87\n","logits_greaterThan = get_logits_for_range(tensor, YY, 99, vocab)\n","print(logits_greaterThan.shape)  # Should be [batch size, seq len, (99-YY+1)]"],"metadata":{"id":"tD_ZaBNug7l1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logits_greaterThan_sum = logits_greaterThan.sum(dim=1)\n","logits_greaterThan_sum.size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RoCg3CGsiuoC","executionInfo":{"status":"ok","timestamp":1698589446079,"user_tz":240,"elapsed":7,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"8ca7df7a-18f7-4cbf-dffe-c405c8789c4b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([32])"]},"metadata":{},"execution_count":145}]},{"cell_type":"code","source":["logits_greaterThan_sum.mean()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WEWYNyHhhFqx","executionInfo":{"status":"ok","timestamp":1698589446489,"user_tz":240,"elapsed":5,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"bc775a4b-5a73-489e-88c3-d56f4e4022f0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(-0.2856)"]},"metadata":{},"execution_count":146}]},{"cell_type":"code","source":["# int(prompts[input_ind]['YY'])  # YY in an input\n","# we want the first token > int(prompts[batch_ind]['YY'])\n","# search thru entire vocab space until find token > int(prompts[input_ind]['YY'])\n","# how do we convert index in vocab space to the token it represents?\n","\n","# greater_than_Y_idx = (io_logits > Y).nonzero(as_tuple=True)[0].item()\n","# first_token_greater_than_Y = dataset.io_tokenIDs[greater_than_Y_idx]"],"metadata":{"id":"pqActCNEgOYD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Ablation Expm Functions"],"metadata":{"id":"GCCCoO0V7L7J"}},{"cell_type":"code","source":["from torch import Tensor\n","\n","def logits_to_ave_logit_diff_2(logits: Float[Tensor, \"batch seq d_vocab\"], dataset: Dataset, per_prompt=False):\n","    '''\n","    Returns logit difference between the correct and incorrect answer.\n","    If per_prompt=True, return the array of differences rather than the average.\n","    '''\n","\n","    # Get the right logits; anything greater than YY\n","    # range(logits.size(0)) for every input in the batch\n","    # dataset.word_idx[\"end\"]: at the last pos, so \"what's the next prediction after end?\"\n","    # what's the logit of the YY token (whose pos at an input seq is recorded in the dataset by dataset.YY_tokenIDs)\n","\n","    # YY = dataset.YY  # only correct dataset indices of corr and incorr tokens matters\n","    YY = 50\n","\n","    logits_greaterThan = get_logits_for_range(logits, YY, 99, vocab)\n","    logits_greaterThan_sum = logits_greaterThan.sum(dim=1)\n","\n","    # get the wrong logits; anything less than YY\n","    logits_lessThan = get_logits_for_range(logits, 00, YY-1, vocab)\n","    logits_lessThan_sum = logits_lessThan.sum(dim=1)\n","\n","    # Find logit difference of corr minus incorr; sum up all tokens between YY and 99, minus sum of all YY and 00\n","    answer_logit_diff = logits_greaterThan_sum - logits_lessThan_sum\n","    return answer_logit_diff if per_prompt else answer_logit_diff.mean()"],"metadata":{"id":"CgD41x5nbKKP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def mean_ablate_by_lst(lst, model, print_output=True):\n","    # CIRCUIT = {\n","    #     \"number mover\": lst,\n","    #     \"number mover 2\": lst,\n","    # }\n","\n","    # SEQ_POS_TO_KEEP = {\n","    #     \"number mover\": \"end\",\n","    #     \"number mover 2\": \"YY\",\n","    # }\n","    CIRCUIT = {}\n","    SEQ_POS_TO_KEEP = {}\n","\n","    for ind, key in enumerate(pos_dict.keys()):\n","        headName = \"head\" + str(ind)\n","        CIRCUIT[headName] = lst\n","        SEQ_POS_TO_KEEP[headName] = key\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)  # make sure text in clean vs corr have same num tokens for each prompt\n","\n","    orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    if print_output:\n","        # print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        # print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    return new_score"],"metadata":{"id":"LqsdFmbVMntG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## test fns"],"metadata":{"id":"o56Altbhz7i6"}},{"cell_type":"code","source":["CIRCUIT = {}\n","SEQ_POS_TO_KEEP = {}\n","\n","for ind, key in enumerate(pos_dict.keys()):\n","    headName = \"head\" + str(ind)\n","    CIRCUIT[headName] = [(layer, head) for layer in range(12) for head in range(12)]\n","    SEQ_POS_TO_KEEP[headName] = key"],"metadata":{"id":"AeWAW2p00Egu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","ioi_logits_minimal = model(dataset.toks)"],"metadata":{"id":"DvmK4HFx0As0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["YY = 50\n","\n","logits_greaterThan = get_logits_for_range(ioi_logits_original, YY, 99, vocab)\n","logits_greaterThan_sum = logits_greaterThan.sum(dim=1)\n","\n","# get the wrong logits; anything less than YY\n","logits_lessThan = get_logits_for_range(ioi_logits_original, 00, YY-1, vocab)\n","logits_lessThan_sum = logits_lessThan.sum(dim=1)\n","\n","# Find logit difference of corr minus incorr; sum up all tokens between YY and 99, minus sum of all YY and 00\n","answer_logit_diff = logits_greaterThan_sum - logits_lessThan_sum"],"metadata":{"id":"sJRHYC1nyax_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logits_greaterThan.size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Rvqf5cLyg4u","executionInfo":{"status":"ok","timestamp":1698589713438,"user_tz":240,"elapsed":448,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"0d01132f-bf90-416c-e13a-9707f43d8b52"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 50])"]},"metadata":{},"execution_count":160}]},{"cell_type":"markdown","source":["# Ablate the model tests"],"metadata":{"id":"Lk3bffnCYq-p"}},{"cell_type":"markdown","source":["## Test Greater-Than vs other circuits"],"metadata":{"id":"GyMsBAOg5kEj"}},{"cell_type":"markdown","source":["See how greater-than circuit performs on the greater-than task; it should be similar to the paper. Else, either greater-than paper has issues (less likely) or this mean ablation code/setup was not generalized correctly (more likely)."],"metadata":{"id":"Y1JSjFGW4sSw"}},{"cell_type":"code","source":["greater_than = [(0, 1), (0, 3), (0, 5), (5, 5), (6, 1), (6, 9), (7, 10), (8, 11), (9, 1)]\n","new_score = mean_ablate_by_lst(greater_than, model, print_output=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WIDjQxvG8tTh","executionInfo":{"status":"ok","timestamp":1698590166716,"user_tz":240,"elapsed":1563,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"a73d3edf-d2ba-4242-a279-a2e9c9ad4927"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (circuit / full) %: 85.1862\n"]}]},{"cell_type":"markdown","source":["Did it get right? Let's try an incompelte circuit for sanity check."],"metadata":{"id":"49nxie05eRMF"}},{"cell_type":"code","source":["greater_than = []\n","new_score = mean_ablate_by_lst(greater_than, model, print_output=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sy7MVh7jkpND","executionInfo":{"status":"ok","timestamp":1698589731690,"user_tz":240,"elapsed":1152,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"436fd368-58d9-4af3-c8fc-15ef24179f0b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (circuit / full) %: 1.8475\n"]}]},{"cell_type":"markdown","source":["Likely still has score due to MLPs"],"metadata":{"id":"KmMRkLIc204W"}},{"cell_type":"code","source":["greater_than = [(0, 1)]\n","new_score = mean_ablate_by_lst(greater_than, model, print_output=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OmnL1aG4eas5","executionInfo":{"status":"ok","timestamp":1698589735684,"user_tz":240,"elapsed":1380,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"988696ef-01de-4690-9d51-6c0d13a7b086"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (circuit / full) %: 1.8473\n"]}]},{"cell_type":"code","source":["greater_than = [(0, 1), (0, 3), (9,1)]\n","new_score = mean_ablate_by_lst(greater_than, model, print_output=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WMcTXbkveQeD","executionInfo":{"status":"ok","timestamp":1698589738110,"user_tz":240,"elapsed":936,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"355ea8b9-8a60-404a-99c8-1b91e3b87833"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (circuit / full) %: 30.9436\n"]}]},{"cell_type":"code","source":["import random\n","num_of_tuples = 9  # Number of tuples you want\n","greater_than = [(random.randint(0, 9), random.randint(0, 9)) for _ in range(num_of_tuples)]\n","new_score = mean_ablate_by_lst(greater_than, model, print_output=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rsxdwJCEyCs5","executionInfo":{"status":"ok","timestamp":1698589924354,"user_tz":240,"elapsed":1638,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"b3262f5c-6fd9-47f1-d784-9f400e7819e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (circuit / full) %: 2.1236\n"]}]},{"cell_type":"code","source":["greater_than = [(layer, head) for layer in range(12) for head in range(12)]\n","new_score = mean_ablate_by_lst(greater_than, model, print_output=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0iD62A1k2Gtf","executionInfo":{"status":"ok","timestamp":1698589883479,"user_tz":240,"elapsed":1398,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"fe9ac576-dbea-4453-a9ad-6f1ee7dcacc0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (circuit / full) %: 100.0000\n"]}]},{"cell_type":"markdown","source":["### add heads to orig paper circ"],"metadata":{"id":"7Hl3NU_i29Sa"}},{"cell_type":"code","source":["greater_than = [(0, 1), (0, 3), (0, 5), (5, 5), (6, 1), (6, 9), (7, 10), (8, 11), (9,1)] + [(10, 7)]\n","new_score = mean_ablate_by_lst(greater_than, model, print_output=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"52p8CGbg2_h3","executionInfo":{"status":"ok","timestamp":1698590186309,"user_tz":240,"elapsed":1894,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"9a4d6f37-6be1-4295-a502-a4c048d7a397"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (circuit / full) %: 84.0478\n"]}]},{"cell_type":"code","source":["greater_than = [(0, 1), (0, 3), (0, 5), (5, 5), (6, 1), (6, 9), (7, 10), (8, 11), (9,1)] + [(layer, head) for layer in range(0) for head in range(12)]\n","new_score = mean_ablate_by_lst(greater_than, model, print_output=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PqonWDSj3yby","executionInfo":{"status":"ok","timestamp":1698590335787,"user_tz":240,"elapsed":2227,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"6a5c0e49-e788-4d0f-d9e2-198e4ddf9465"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (circuit / full) %: 85.1862\n"]}]},{"cell_type":"code","source":["greater_than = [(0, 1), (0, 3), (0, 5), (5, 5), (6, 1), (6, 9), (7, 10), (8, 11), (9,1)] + [(layer, head) for layer in range(0, 4) for head in range(12)]\n","new_score = mean_ablate_by_lst(greater_than, model, print_output=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QjyqSN0p361x","executionInfo":{"status":"ok","timestamp":1698590354545,"user_tz":240,"elapsed":2400,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"a8a7158c-aaa4-456b-e2ba-ac7648eed80e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (circuit / full) %: 81.5903\n"]}]},{"cell_type":"code","source":["greater_than = [(0, 1), (0, 3), (0, 5), (5, 5), (6, 1), (6, 9), (7, 10), (8, 11), (9,1)] + [(layer, head) for layer in range(0, 6) for head in range(12)]\n","new_score = mean_ablate_by_lst(greater_than, model, print_output=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q6AGUcI73_8u","executionInfo":{"status":"ok","timestamp":1698590372089,"user_tz":240,"elapsed":2787,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"7e235d38-f1a8-4c35-dc51-62c01f832ca8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (circuit / full) %: 88.9415\n"]}]},{"cell_type":"code","source":["greater_than = [(0, 1), (0, 3), (0, 5), (5, 5), (6, 1), (6, 9), (7, 10), (8, 11), (9,1)] + [(layer, head) for layer in range(0, 9) for head in range(12)]\n","new_score = mean_ablate_by_lst(greater_than, model, print_output=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IdKTtmOO4EHQ","executionInfo":{"status":"ok","timestamp":1698590387390,"user_tz":240,"elapsed":1825,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"cfd93fcd-10fd-4ef2-d170-415ff5639242"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (circuit / full) %: 97.0480\n"]}]},{"cell_type":"code","source":["greater_than = [(0, 1), (0, 3), (0, 5), (5, 5), (6, 1), (6, 9), (7, 10), (8, 11), (9,1)] + [(layer, head) for layer in range(8, 9) for head in range(12)]\n","new_score = mean_ablate_by_lst(greater_than, model, print_output=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4M-CxPxP4IJf","executionInfo":{"status":"ok","timestamp":1698590402640,"user_tz":240,"elapsed":2126,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"933c4d60-4e63-4ac5-9ad9-ec991d8287b3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (circuit / full) %: 92.0383\n"]}]},{"cell_type":"code","source":["greater_than = [(0, 1), (0, 3), (0, 5), (5, 5), (6, 1), (6, 9), (7, 10), (8, 11), (9,1)] + [(layer, head) for layer in range(5, 9) for head in range(12)]\n","new_score = mean_ablate_by_lst(greater_than, model, print_output=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fBM_g-Eu4NZf","executionInfo":{"status":"ok","timestamp":1698590422803,"user_tz":240,"elapsed":1299,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"8883c9f7-27f7-43e7-f8dc-825b403382ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (circuit / full) %: 97.0697\n"]}]},{"cell_type":"markdown","source":["# Ablate by seq pos"],"metadata":{"id":"o7d6V0Cz5hV3"}},{"cell_type":"code","source":["def mean_ablate_by_seqpos(lst, model, print_output=True):\n","    # CIRCUIT = {\n","    #     \"number mover\": lst,\n","    #     \"number mover 2\": lst,\n","    # }\n","\n","    # SEQ_POS_TO_KEEP = {\n","    #     \"number mover\": \"end\",\n","    #     \"number mover 2\": \"YY\",\n","    # }\n","    CIRCUIT = {}\n","    SEQ_POS_TO_KEEP = {}\n","\n","    # for ind, key in enumerate(pos_dict.keys()):\n","    # ind = 7\n","    # key = \"T7\"\n","\n","    for ind, key in enumerate([\"T7\", \"end\"]):\n","        headName = \"head\" + str(ind)\n","        CIRCUIT[headName] = lst\n","        SEQ_POS_TO_KEEP[headName] = key\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)  # make sure text in clean vs corr have same num tokens for each prompt\n","\n","    orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    if print_output:\n","        # print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        # print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    return new_score"],"metadata":{"id":"_4hXcIcZ5jOU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["greater_than = [(0, 1), (0, 3), (0, 5), (5, 5), (6, 1), (6, 9), (7, 10), (8, 11), (9, 1)]\n","new_score = mean_ablate_by_seqpos(greater_than, model, print_output=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dxYqvBnr5vIr","executionInfo":{"status":"ok","timestamp":1698590903797,"user_tz":240,"elapsed":2405,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"330886ab-16e9-40bc-ae79-d16ec1ad6363"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (circuit / full) %: 85.1939\n"]}]},{"cell_type":"code","source":["def mean_ablate_by_seqpos(lst, model, print_output=True):\n","    CIRCUIT = {\n","        \"end\": [(0, 1), (0, 3), (0, 5), (5, 5), (6, 1), (6, 9), (7, 10), (8, 11), (9, 1)],\n","        \"YY\": [(0, 1), (0, 3), (0, 5), (5, 5), (6, 1), (6, 9)],\n","    }\n","\n","    SEQ_POS_TO_KEEP = {\n","        \"end\": \"end\",\n","        \"YY\": \"T7\",\n","    }\n","    # CIRCUIT = {}\n","    # SEQ_POS_TO_KEEP = {}\n","\n","    # # for ind, key in enumerate(pos_dict.keys()):\n","    # # ind = 7\n","    # # key = \"T7\"\n","\n","    # for ind, key in enumerate([\"T7\", \"end\"]):\n","    #     headName = \"head\" + str(ind)\n","    #     CIRCUIT[headName] = lst\n","    #     SEQ_POS_TO_KEEP[headName] = key\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)  # make sure text in clean vs corr have same num tokens for each prompt\n","\n","    orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    if print_output:\n","        # print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        # print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    return new_score"],"metadata":{"id":"nFwZO0nq6J94"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["greater_than = [(0, 1), (0, 3), (0, 5), (5, 5), (6, 1), (6, 9), (7, 10), (8, 11), (9, 1)]\n","new_score = mean_ablate_by_seqpos(greater_than, model, print_output=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698591031785,"user_tz":240,"elapsed":465,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"92ee5345-3c4d-47c5-acc3-b50dfdd417bc","id":"sWYJpyBB6J95"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (circuit / full) %: 85.3446\n"]}]},{"cell_type":"code","source":["def mean_ablate_by_seqpos(lst, model, print_output=True):\n","    CIRCUIT = {\n","        \"end\": [(9, 1)],\n","        \"YY\": [(0, 1), (0, 3), (0, 5), (5, 5), (6, 1), (6, 9), (7, 10), (8, 11)],\n","    }\n","\n","    SEQ_POS_TO_KEEP = {\n","        \"end\": \"end\",\n","        \"YY\": \"T7\",\n","    }\n","    # CIRCUIT = {}\n","    # SEQ_POS_TO_KEEP = {}\n","\n","    # # for ind, key in enumerate(pos_dict.keys()):\n","    # # ind = 7\n","    # # key = \"T7\"\n","\n","    # for ind, key in enumerate([\"T7\", \"end\"]):\n","    #     headName = \"head\" + str(ind)\n","    #     CIRCUIT[headName] = lst\n","    #     SEQ_POS_TO_KEEP[headName] = key\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)  # make sure text in clean vs corr have same num tokens for each prompt\n","\n","    orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    if print_output:\n","        # print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        # print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    return new_score"],"metadata":{"id":"CffrMRgt6klD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["greater_than = [(0, 1), (0, 3), (0, 5), (5, 5), (6, 1), (6, 9), (7, 10), (8, 11), (9, 1)]\n","new_score = mean_ablate_by_seqpos(greater_than, model, print_output=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698591089292,"user_tz":240,"elapsed":1426,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"6c38a523-242d-4bcb-dea3-d324d01fbe22","id":"OO0m6DkS6klE"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (circuit / full) %: 43.3762\n"]}]},{"cell_type":"code","source":["def mean_ablate_by_seqpos(lst, model, print_output=True):\n","    CIRCUIT = {\n","        \"end\": [(0, 1), (0, 3), (0, 5), (5, 5), (6, 1), (6, 9), (7, 10), (8, 11), (9, 1)],\n","        # \"YY\": [(0, 1), (0, 3), (0, 5), (5, 5), (6, 1), (6, 9), (7, 10), (8, 11)],\n","    }\n","\n","    SEQ_POS_TO_KEEP = {\n","        \"end\": \"end\",\n","        # \"YY\": \"T7\",\n","    }\n","    # CIRCUIT = {}\n","    # SEQ_POS_TO_KEEP = {}\n","\n","    # # for ind, key in enumerate(pos_dict.keys()):\n","    # # ind = 7\n","    # # key = \"T7\"\n","\n","    # for ind, key in enumerate([\"T7\", \"end\"]):\n","    #     headName = \"head\" + str(ind)\n","    #     CIRCUIT[headName] = lst\n","    #     SEQ_POS_TO_KEEP[headName] = key\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)  # make sure text in clean vs corr have same num tokens for each prompt\n","\n","    orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    if print_output:\n","        # print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        # print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    return new_score"],"metadata":{"id":"pQ-PxR5663gS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["greater_than = [(0, 1), (0, 3), (0, 5), (5, 5), (6, 1), (6, 9), (7, 10), (8, 11), (9, 1)]\n","new_score = mean_ablate_by_seqpos(greater_than, model, print_output=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698591137002,"user_tz":240,"elapsed":4421,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"fd99831b-d504-43fc-9568-4d81f409a465","id":"SMkzD9rd63gg"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (circuit / full) %: 72.7873\n"]}]},{"cell_type":"markdown","source":["# Prune backwards"],"metadata":{"id":"e6N5MU1wRZog"}},{"cell_type":"code","source":["# Start with full circuit\n","curr_circuit = [(layer, head) for layer in range(12) for head in range(12)]\n","threshold = 3  # This is T, a %. if performance is less than T%, allow its removal\n","\n","for layer in range(11, -1, -1):  # go thru all heads in a layer first\n","    for head in range(12):\n","        # Copying the curr_circuit so we can iterate over one and modify the other\n","        copy_circuit = curr_circuit.copy()\n","\n","        # Temporarily removing the current tuple from the copied circuit\n","        copy_circuit.remove((layer, head))\n","\n","        new_score = mean_ablate_by_lst(copy_circuit, model, print_output=False).item()\n","\n","        # print((layer,head), new_score)\n","        # If the result is less than the threshold, remove the tuple from the original list\n","        if (100 - new_score) < threshold:\n","            curr_circuit.remove((layer, head))\n","\n","            print(\"Removed:\", (layer, head))\n","            print(new_score)\n","            print(\"\\n\")"],"metadata":{"id":"Bfiwe5d3SgVP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mean_ablate_by_lst(curr_circuit, model, print_output=True)"],"metadata":{"id":"qgpGMTWLbibq","executionInfo":{"status":"ok","timestamp":1698518383930,"user_tz":240,"elapsed":2462,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2d907a0f-b655-4076-d280-4cf3259a5853"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average logit difference (circuit / full) %: 103.1818\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor(103.1818, device='cuda:0')"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["backw_3 = curr_circuit.copy()\n","backw_3"],"metadata":{"id":"7_ZC4-k2blg2","executionInfo":{"status":"ok","timestamp":1698518383930,"user_tz":240,"elapsed":13,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9478e40f-c388-40e2-e119-ea1609b80eda"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(0, 0),\n"," (0, 1),\n"," (0, 2),\n"," (0, 3),\n"," (0, 5),\n"," (0, 9),\n"," (0, 10),\n"," (1, 0),\n"," (1, 1),\n"," (1, 5),\n"," (1, 6),\n"," (1, 7),\n"," (1, 8),\n"," (2, 2),\n"," (2, 8),\n"," (2, 10),\n"," (3, 0),\n"," (3, 3),\n"," (3, 7),\n"," (3, 8),\n"," (4, 3),\n"," (4, 4),\n"," (4, 6),\n"," (4, 7),\n"," (4, 8),\n"," (4, 9),\n"," (4, 10),\n"," (4, 11),\n"," (5, 0),\n"," (5, 2),\n"," (5, 3),\n"," (5, 4),\n"," (5, 6),\n"," (5, 8),\n"," (5, 11),\n"," (6, 1),\n"," (6, 3),\n"," (6, 9),\n"," (6, 10),\n"," (6, 11),\n"," (7, 0),\n"," (7, 9),\n"," (7, 10),\n"," (7, 11),\n"," (8, 11),\n"," (9, 1),\n"," (11, 10)]"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["len(backw_3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dry8ANsOMgvg","executionInfo":{"status":"ok","timestamp":1698518383930,"user_tz":240,"elapsed":9,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"17d99dc5-46a8-4a84-c45b-0f282f5ea2fa"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["47"]},"metadata":{},"execution_count":25}]}]}