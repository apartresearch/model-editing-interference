{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcZG9rm2IAiA"
      },
      "source": [
        "# Setup\n",
        "(No need to change anything)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "rMcpSDdjIAiA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e6ad056-c833-448d-c176-5324a446e3cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running as a Colab notebook\n",
            "Collecting git+https://github.com/neelnanda-io/TransformerLens.git\n",
            "  Cloning https://github.com/neelnanda-io/TransformerLens.git to /tmp/pip-req-build-sa_v46nq\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/TransformerLens.git /tmp/pip-req-build-sa_v46nq\n",
            "  Resolved https://github.com/neelnanda-io/TransformerLens.git to commit 0825c5eb4196e7ad72d28bcf4e615306b3897490\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (0.25.0)\n",
            "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (0.14.1)\n",
            "Requirement already satisfied: datasets>=2.7.1 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (2.15.0)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (0.7.0)\n",
            "Requirement already satisfied: fancy-einsum>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (0.0.3)\n",
            "Requirement already satisfied: jaxtyping>=0.2.11 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (0.2.24)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (1.26.2)\n",
            "Requirement already satisfied: nvidia-cublas-cu12>=12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12>=12.1.105 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12>=12.1.105 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12>=12.1.105 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12>=8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cufft-cu12>=11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12>=10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12>=11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12>=12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12>=2.18.1 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12>=12.1.105 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (12.1.105)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (1.5.3)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (13.7.0)\n",
            "Requirement already satisfied: torch!=2.0,!=2.1.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (2.1.1)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (4.66.1)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (4.35.2)\n",
            "Requirement already satisfied: triton>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (2.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (4.5.0)\n",
            "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (0.16.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (0.19.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (0.4.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (9.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (0.3.7)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (2.31.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (3.9.1)\n",
            "Requirement already satisfied: typeguard<3,>=2.13.3 in /usr/local/lib/python3.10/dist-packages (from jaxtyping>=0.2.11->transformer-lens==0.0.0) (2.13.3)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12>=11.4.5.107->transformer-lens==0.0.0) (12.3.101)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2023.3.post1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer-lens==0.0.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer-lens==0.0.0) (2.16.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (3.1.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->transformer-lens==0.0.0) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->transformer-lens==0.0.0) (0.15.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (3.1.40)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (1.38.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (0.4.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer-lens==0.0.0) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (4.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens==0.0.0) (4.0.11)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer-lens==0.0.0) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (1.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens==0.0.0) (5.0.1)\n"
          ]
        }
      ],
      "source": [
        "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
        "DEBUG_MODE = False\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running as a Colab notebook\")\n",
        "    %pip install git+https://github.com/neelnanda-io/TransformerLens.git\n",
        "    # Install another version of node that makes PySvelte work way faster\n",
        "    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
        "    # %pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
        "    from IPython import get_ipython\n",
        "\n",
        "    ipython = get_ipython()\n",
        "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
        "    ipython.magic(\"load_ext autoreload\")\n",
        "    ipython.magic(\"autoreload 2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Z6b1n2tvIAiD"
      },
      "outputs": [],
      "source": [
        "# Import stuff\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import einops\n",
        "from fancy_einsum import einsum\n",
        "import tqdm.notebook as tqdm\n",
        "import random\n",
        "from pathlib import Path\n",
        "import plotly.express as px\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from jaxtyping import Float, Int\n",
        "from typing import List, Union, Optional\n",
        "from functools import partial\n",
        "import copy\n",
        "\n",
        "import itertools\n",
        "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
        "import dataclasses\n",
        "import datasets\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "zuhzYxbsIAiE"
      },
      "outputs": [],
      "source": [
        "import transformer_lens\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens.hook_points import (\n",
        "    HookedRootModule,\n",
        "    HookPoint,\n",
        ")  # Hooking utilities\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hccba0v-IAiF"
      },
      "source": [
        "We turn automatic differentiation off, to save GPU memory, as this notebook focuses on model inference not model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "cFMTUcQiIAiF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71be493c-5db6-4500-81da-71aa57ede0dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x7d8c1b553a60>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "torch.set_grad_enabled(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model"
      ],
      "metadata": {
        "id": "OLkInsdjyHMx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "xLwDyosvIAiJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e01706b1-27ba-48b8-e4ab-9d7cd5b0532e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained model gpt2-small into HookedTransformer\n"
          ]
        }
      ],
      "source": [
        "model = HookedTransformer.from_pretrained(\n",
        "    \"gpt2-small\",\n",
        "    center_unembed=True,\n",
        "    center_writing_weights=True,\n",
        "    fold_ln=True,\n",
        "    refactor_factored_attn_matrices=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import functions from repo"
      ],
      "metadata": {
        "id": "Z4iJEGh6b56v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/callummcdougall/ARENA_2.0.git"
      ],
      "metadata": {
        "id": "Fdh5--MfYw7-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "463432b6-b3a9-4b2d-e25c-74fb16e92140"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ARENA_2.0'...\n",
            "remote: Enumerating objects: 9262, done.\u001b[K\n",
            "remote: Counting objects: 100% (1976/1976), done.\u001b[K\n",
            "remote: Compressing objects: 100% (344/344), done.\u001b[K\n",
            "remote: Total 9262 (delta 1727), reused 1731 (delta 1628), pack-reused 7286\u001b[K\n",
            "Receiving objects: 100% (9262/9262), 156.79 MiB | 15.30 MiB/s, done.\n",
            "Resolving deltas: 100% (5620/5620), done.\n",
            "Updating files: 100% (387/387), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd ARENA_2.0/chapter1_transformers/exercises/part3_indirect_object_identification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhmqg_vsng4S",
        "outputId": "a4f4f7b9-a18c-4144-e466-0e87cbbd50f8"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ARENA_2.0/chapter1_transformers/exercises/part3_indirect_object_identification/ARENA_2.0/chapter1_transformers/exercises/part3_indirect_object_identification\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ioi_circuit_extraction as ioi_circuit_extraction"
      ],
      "metadata": {
        "id": "OT0Sn571ZnkV"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ioi_dataset import IOIDataset"
      ],
      "metadata": {
        "id": "QfLqx5QkO5hV"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# test"
      ],
      "metadata": {
        "id": "Q74PH3CiSgw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_batch_dim(\n",
        "    tensor: Float[torch.Tensor, \"1 ...\"]\n",
        ") -> Float[torch.Tensor, \"...\"]:\n",
        "    \"\"\"\n",
        "    Removes the first dimension of a tensor if it is size 1, otherwise returns the tensor unchanged\n",
        "    \"\"\"\n",
        "    if tensor.shape[0] == 1:\n",
        "        return tensor.squeeze(0)\n",
        "    else:\n",
        "        return tensor\n",
        "\n",
        "def test_prompt(\n",
        "    prompt: str,\n",
        "    answer: str,\n",
        "    model,\n",
        "    orig_model,\n",
        "    prepend_space_to_answer: bool = True,\n",
        "    print_details: bool = True,\n",
        "    prepend_bos: bool = True,\n",
        "    top_k: int = 10,\n",
        "):\n",
        "    \"\"\"\n",
        "    Function to test whether a model can give the correct answer to a prompt. Intended for exploratory analysis, so it prints things out rather than returning things.\n",
        "\n",
        "    Works for multi-token answers and multi-token prompts.\n",
        "\n",
        "    Will always print the ranks of the answer tokens, and if print_details will print the logit and prob for the answer tokens and the top k tokens returned for each answer position.\n",
        "    \"\"\"\n",
        "    if prepend_space_to_answer and not answer.startswith(\" \"):\n",
        "        answer = \" \" + answer\n",
        "    # GPT-2 often treats the first token weirdly, so lets give it a resting position\n",
        "    tokens = orig_model.to_tokens(prompt + answer, prepend_bos=prepend_bos)\n",
        "\n",
        "\n",
        "    prompt_str_tokens = orig_model.to_str_tokens(prompt, prepend_bos=prepend_bos)\n",
        "    answer_str_tokens = orig_model.to_str_tokens(answer, prepend_bos=False)\n",
        "    prompt_length = len(prompt_str_tokens)\n",
        "    answer_length = len(answer_str_tokens)\n",
        "    if print_details:\n",
        "        print(\"Tokenized prompt:\", prompt_str_tokens)\n",
        "        print(\"Tokenized answer:\", answer_str_tokens)\n",
        "\n",
        "    logits = remove_batch_dim(model(tokens))\n",
        "\n",
        "    probs = logits.softmax(dim=-1)\n",
        "    answer_ranks = []\n",
        "    for index in range(prompt_length, prompt_length + answer_length):\n",
        "        answer_token = tokens[0, index]\n",
        "        answer_str_token = answer_str_tokens[index - prompt_length]\n",
        "        # Offset by 1 because models predict the NEXT token\n",
        "        token_probs = probs[index - 1]\n",
        "        sorted_token_probs, sorted_token_values = token_probs.sort(descending=True) # sorted_token_values are the indices; the indices correspond to token num of vocab\n",
        "        # Janky way to get the index of the token in the sorted list - I couldn't find a better way?\n",
        "        correct_rank = torch.arange(len(sorted_token_values))[\n",
        "            (sorted_token_values == answer_token).cpu()\n",
        "        ].item()\n",
        "        answer_ranks.append((answer_str_token, correct_rank))\n",
        "        if print_details:\n",
        "            # String formatting syntax - the first number gives the number of characters to pad to, the second number gives the number of decimal places.\n",
        "            # rprint gives rich text printing\n",
        "            print(\n",
        "                f\"Performance on answer token:\\n[b]Rank: {correct_rank: <8} Logit: {logits[index-1, answer_token].item():5.2f} Prob: {token_probs[answer_token].item():6.2%} Token: |{answer_str_token}|[/b]\"\n",
        "            )\n",
        "            for i in range(top_k):\n",
        "                print(\n",
        "                    f\"Top {i}th token. Logit: {logits[index-1, sorted_token_values[i]].item():5.2f} Prob: {sorted_token_probs[i].item():6.2%} Token: |{orig_model.to_string(sorted_token_values[i])}|\"\n",
        "                )\n",
        "    print(f\"[b]Ranks of the answer tokens:[/b] {answer_ranks}\")"
      ],
      "metadata": {
        "id": "Hba_G6MBShaH"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"1 2 3 4\"\n",
        "answer = \" 5\"\n",
        "\n",
        "prompt_str_tokens = model.to_str_tokens(prompt, prepend_bos=True)\n",
        "answer_str_tokens = model.to_str_tokens(answer, prepend_bos=False)\n",
        "prompt_length = len(prompt_str_tokens)\n",
        "answer_length = len(answer_str_tokens)\n",
        "\n",
        "print(\"Tokenized prompt:\", prompt_str_tokens)\n",
        "print(\"Tokenized answer:\", answer_str_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SdB_gJ2YVjc",
        "outputId": "b2e6253b-c7c1-404b-8e51-de628f26dcdf"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized prompt: ['<|endoftext|>', '1', ' 2', ' 3', ' 4']\n",
            "Tokenized answer: [' 5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.reset_hooks(including_permanent=True)\n",
        "# tokens = model.to_tokens(prompt)\n",
        "tokens = model.to_tokens(prompt + answer, prepend_bos=True)"
      ],
      "metadata": {
        "id": "p3HrKuNqTGUd"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits = model(tokens)"
      ],
      "metadata": {
        "id": "gag2c_g5XbLq"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QWg04azXsEl",
        "outputId": "ba8a4661-de9c-4536-9896-e1459069bd95"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 6, 50257])"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits = logits.squeeze(0)\n",
        "logits.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXykhgaOX1Yt",
        "outputId": "73993dd7-f9ee-40b6-f144-ca2a2fd79fbb"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6, 50257])"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 6\n",
        "\n",
        "probs = logits.softmax(dim=-1)\n",
        "answer_ranks = []\n",
        "for index in range(prompt_length, prompt_length + answer_length):\n",
        "    answer_token = tokens[0, index]\n",
        "    answer_str_token = answer_str_tokens[index - prompt_length]\n",
        "    # Offset by 1 because models predict the NEXT token\n",
        "    token_probs = probs[index - 1]\n",
        "    sorted_token_probs, sorted_token_values = token_probs.sort(descending=True) # sorted_token_values are the indices; the indices correspond to token num of vocab\n",
        "    # Janky way to get the index of the token in the sorted list - I couldn't find a better way?\n",
        "    correct_rank = torch.arange(len(sorted_token_values))[\n",
        "        (sorted_token_values == answer_token).cpu()\n",
        "    ].item()\n",
        "    answer_ranks.append((answer_str_token, correct_rank))\n",
        "\n",
        "    # String formatting syntax - the first number gives the number of characters to pad to, the second number gives the number of decimal places.\n",
        "    # rprint gives rich text printing\n",
        "    print(\n",
        "        f\"Performance on answer token:\\n[b]Rank: {correct_rank: <8} Logit: {logits[index-1, answer_token].item():5.2f} Prob: {token_probs[answer_token].item():6.2%} Token: |{answer_str_token}|[/b]\"\n",
        "    )\n",
        "    for i in range(top_k):\n",
        "        print(\n",
        "            f\"Top {i}th token. Logit: {logits[index-1, sorted_token_values[i]].item():5.2f} Prob: {sorted_token_probs[i].item():6.2%} Token: |{model.to_string(sorted_token_values[i])}|\"\n",
        "        )\n",
        "print(f\"[b]Ranks of the answer tokens:[/b] {answer_ranks}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3l412M9zTdGO",
        "outputId": "0210e6f1-2e0d-4b63-a982-351627dfff87"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance on answer token:\n",
            "[b]Rank: 0        Logit: 18.76 Prob: 96.18% Token: | 5|[/b]\n",
            "Top 0th token. Logit: 18.76 Prob: 96.18% Token: | 5|\n",
            "Top 1th token. Logit: 13.27 Prob:  0.40% Token: | Next|\n",
            "Top 2th token. Logit: 13.01 Prob:  0.30% Token: |\n",
            "|\n",
            "Top 3th token. Logit: 12.87 Prob:  0.27% Token: | >|\n",
            "Top 4th token. Logit: 12.04 Prob:  0.12% Token: | 4|\n",
            "Top 5th token. Logit: 11.88 Prob:  0.10% Token: | 50|\n",
            "[b]Ranks of the answer tokens:[/b] [(' 5', 0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate dataset with multiple prompts"
      ],
      "metadata": {
        "id": "6Fuq8XW770vX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset:\n",
        "    def __init__(self, prompts, pos_dict, tokenizer, S1_is_first=False):\n",
        "        self.prompts = prompts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.N = len(prompts)\n",
        "        self.max_len = max(\n",
        "            [\n",
        "                len(self.tokenizer(prompt[\"text\"]).input_ids)\n",
        "                for prompt in self.prompts\n",
        "            ]\n",
        "        )\n",
        "        # all_ids = [prompt[\"TEMPLATE_IDX\"] for prompt in self.ioi_prompts]\n",
        "        all_ids = [0 for prompt in self.prompts] # only 1 template\n",
        "        all_ids_ar = np.array(all_ids)\n",
        "        self.groups = []\n",
        "        for id in list(set(all_ids)):\n",
        "            self.groups.append(np.where(all_ids_ar == id)[0])\n",
        "\n",
        "        texts = [ prompt[\"text\"] for prompt in self.prompts ]\n",
        "        self.toks = torch.Tensor(self.tokenizer(texts, padding=True).input_ids).type(\n",
        "            torch.int\n",
        "        )\n",
        "        self.corr_tokenIDs = [\n",
        "            self.tokenizer.encode(\" \" + prompt[\"corr\"])[0] for prompt in self.prompts\n",
        "        ]\n",
        "        self.incorr_tokenIDs = [\n",
        "            self.tokenizer.encode(\" \" + prompt[\"incorr\"])[0] for prompt in self.prompts\n",
        "        ]\n",
        "\n",
        "        # word_idx: for every prompt, find the token index of each target token and \"end\"\n",
        "        # word_idx is a tensor with an element for each prompt. The element is the targ token's ind at that prompt\n",
        "        self.word_idx = {}\n",
        "        for targ in [key for key in self.prompts[0].keys() if (key != 'text' and key != 'corr' and key != 'incorr')]:\n",
        "            targ_lst = []\n",
        "            for prompt in self.prompts:\n",
        "                input_text = prompt[\"text\"]\n",
        "                tokens = model.tokenizer.tokenize(input_text)\n",
        "                # if S1_is_first and targ == \"S1\":  # only use this if first token doesn't have space Ġ in front\n",
        "                #     target_token = prompt[targ]\n",
        "                # else:\n",
        "                #     target_token = \"Ġ\" + prompt[targ]\n",
        "                # target_index = tokens.index(target_token)\n",
        "                target_index = pos_dict[targ]\n",
        "                targ_lst.append(target_index)\n",
        "            self.word_idx[targ] = torch.tensor(targ_lst)\n",
        "\n",
        "        targ_lst = []\n",
        "        for prompt in self.prompts:\n",
        "            input_text = prompt[\"text\"]\n",
        "            tokens = self.tokenizer.tokenize(input_text)\n",
        "            end_token_index = len(tokens) - 1\n",
        "            targ_lst.append(end_token_index)\n",
        "        self.word_idx[\"end\"] = torch.tensor(targ_lst)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.N"
      ],
      "metadata": {
        "id": "4wXBNWj5FwVn"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_dict = {\n",
        "    'S1': 0,\n",
        "    'S2': 1,\n",
        "    'S3': 2,\n",
        "    'S4': 3,\n",
        "}"
      ],
      "metadata": {
        "id": "kS_Tlrb_70vg"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompts_list(x ,y):\n",
        "    prompts_list = []\n",
        "    for i in range(x, y):\n",
        "        prompt_dict = {\n",
        "            'S1': str(i),\n",
        "            'S2': str(i+1),\n",
        "            'S3': str(i+2),\n",
        "            'S4': str(i+3),\n",
        "            'corr': str(i+4),\n",
        "            'incorr': str(i+3),\n",
        "            'text': f\"{i} {i+1} {i+2} {i+3}\"\n",
        "        }\n",
        "        prompts_list.append(prompt_dict)\n",
        "    return prompts_list\n",
        "\n",
        "prompts_list = generate_prompts_list(1, 101)\n",
        "dataset = Dataset(prompts_list, pos_dict, model.tokenizer, S1_is_first=True)"
      ],
      "metadata": {
        "id": "u0NPSKcZ1iDe"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def generate_prompts_list_corr(x ,y):\n",
        "    prompts_list = []\n",
        "    for i in range(x, y):\n",
        "        r1 = random.randint(1, 100)\n",
        "        r2 = random.randint(1, 100)\n",
        "        while True:\n",
        "            r3 = random.randint(1, 100)\n",
        "            r4 = random.randint(1, 100)\n",
        "            if r4 - 1 != r3:\n",
        "                break\n",
        "        prompt_dict = {\n",
        "            'S1': str(r1),\n",
        "            'S2': str(r2),\n",
        "            'S3': str(r3),\n",
        "            'S4': str(r4),\n",
        "            'corr': str(i+4),\n",
        "            'incorr': str(i+3),\n",
        "            'text': f\"{r1} {r2} {r3} {r4}\"\n",
        "        }\n",
        "        prompts_list.append(prompt_dict)\n",
        "    return prompts_list\n",
        "\n",
        "# prompts_list_2 = generate_prompts_list_corr(1, 101)\n",
        "prompts_list_2 = generate_prompts_list_corr(1, 2)\n",
        "dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer, S1_is_first=True)"
      ],
      "metadata": {
        "id": "dzzLlCqZS_wl"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer, S1_is_first=True)"
      ],
      "metadata": {
        "id": "msu6D4p_feW5"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP ablation fns"
      ],
      "metadata": {
        "id": "G9FQY3H3zkFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Tuple, List\n",
        "from jaxtyping import Float, Bool\n",
        "import torch as t\n",
        "\n",
        "# lst = [(layer, head) for layer in range(12) for head in range(12)]\n",
        "lst = [layer for layer in range(12)]\n",
        "CIRCUIT = {\n",
        "    \"number mover\": lst,\n",
        "    # \"number mover 4\": lst,\n",
        "    \"number mover 3\": lst,\n",
        "    \"number mover 2\": lst,\n",
        "    \"number mover 1\": lst,\n",
        "}\n",
        "\n",
        "SEQ_POS_TO_KEEP = {\n",
        "    \"number mover\": \"end\",\n",
        "    # \"number mover 4\": \"S4\",\n",
        "    \"number mover 3\": \"S3\",\n",
        "    \"number mover 2\": \"S2\",\n",
        "    \"number mover 1\": \"S1\",\n",
        "}\n",
        "\n",
        "def logits_to_ave_logit_diff(logits: Float[Tensor, \"batch seq d_vocab\"], dataset: Dataset, per_prompt=False):\n",
        "    '''\n",
        "    Returns logit difference between the correct and incorrect answer.\n",
        "\n",
        "    If per_prompt=True, return the array of differences rather than the average.\n",
        "    '''\n",
        "\n",
        "    # Only the final logits are relevant for the answer\n",
        "    # Get the logits corresponding to the indirect object / subject tokens respectively\n",
        "    corr_logits: Float[Tensor, \"batch\"] = logits[range(logits.size(0)), dataset.word_idx[\"end\"], dataset.corr_tokenIDs]\n",
        "    incorr_logits: Float[Tensor, \"batch\"] = logits[range(logits.size(0)), dataset.word_idx[\"end\"], dataset.incorr_tokenIDs]\n",
        "    # Find logit difference\n",
        "    answer_logit_diff = corr_logits - incorr_logits\n",
        "    return answer_logit_diff if per_prompt else answer_logit_diff.mean()"
      ],
      "metadata": {
        "id": "tIZoAvLqz1Sd"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_means_by_template_MLP(\n",
        "    means_dataset: Dataset,\n",
        "    model: HookedTransformer\n",
        ") -> Float[Tensor, \"layer batch seq head_idx d_head\"]:\n",
        "    '''\n",
        "    Returns the mean of each head's output over the means dataset. This mean is\n",
        "    computed separately for each group of prompts with the same template (these\n",
        "    are given by means_dataset.groups).\n",
        "    '''\n",
        "    # Cache the outputs of every head\n",
        "    _, means_cache = model.run_with_cache(\n",
        "        means_dataset.toks.long(),\n",
        "        return_type=None,\n",
        "        # names_filter=lambda name: name.endswith(\"z\"),\n",
        "    )\n",
        "    # Create tensor to store means\n",
        "    n_layers, d_model = model.cfg.n_layers, model.cfg.d_model\n",
        "    batch, seq_len = len(means_dataset), means_dataset.max_len\n",
        "    means = t.zeros(size=(n_layers, batch, seq_len, d_model), device=model.cfg.device)\n",
        "\n",
        "    # Get set of different templates for this data\n",
        "    for layer in range(n_layers):\n",
        "        mlp_output_for_this_layer: Float[Tensor, \"batch seq d_model\"] = means_cache[utils.get_act_name(\"mlp_out\", layer)]\n",
        "        for template_group in means_dataset.groups:  # here, we only have one group\n",
        "            mlp_output_for_this_template = mlp_output_for_this_layer[template_group]\n",
        "            # aggregate all batches\n",
        "            mlp_output_means_for_this_template = einops.reduce(mlp_output_for_this_template, \"batch seq d_model -> seq d_model\", \"mean\")\n",
        "            means[layer, template_group] = mlp_output_means_for_this_template\n",
        "            # at layer, each batch ind is tempalte group (a tensor of size seq d_model)\n",
        "            # is assigned the SAME mean, \"mlp_output_means_for_this_template\"\n",
        "\n",
        "    # # Get set of different templates for this data\n",
        "    # for layer in range(model.cfg.n_layers):\n",
        "    #     z_for_this_layer: Float[Tensor, \"batch seq head d_head\"] = means_cache[utils.get_act_name(\"z\", layer)]\n",
        "    #     for template_group in means_dataset.groups:\n",
        "    #         z_for_this_template = z_for_this_layer[template_group]\n",
        "    #         z_means_for_this_template = einops.reduce(z_for_this_template, \"batch seq head d_head -> seq head d_head\", \"mean\")\n",
        "    #         means[layer, template_group] = z_means_for_this_template\n",
        "\n",
        "    return means"
      ],
      "metadata": {
        "id": "McmRZoY7Wudl"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.reset_hooks(including_permanent=True)\n",
        "\n",
        "# Compute the mean of each head's output on the ABC dataset, grouped by template\n",
        "means = compute_means_by_template_MLP(dataset_2, model)\n",
        "means.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M00-ztUFs7xI",
        "outputId": "dea45dee-1f28-4e3a-cef9-64fbb04fb223"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12, 100, 4, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mlp_outputs_and_posns_to_keep(\n",
        "    means_dataset: Dataset,\n",
        "    model: HookedTransformer,\n",
        "    circuit: Dict[str, List[int]],  # Adjusted to hold list of layers instead of (layer, head) tuples\n",
        "    seq_pos_to_keep: Dict[str, str],\n",
        ") -> Dict[int, Bool[Tensor, \"batch seq\"]]:  # Adjusted the return type to \"batch seq\"\n",
        "    '''\n",
        "    Returns a dictionary mapping layers to a boolean mask giving the indices of the\n",
        "    MLP output which *shouldn't* be mean-ablated.\n",
        "\n",
        "    The output of this function will be used for the hook function that does ablation.\n",
        "    '''\n",
        "    mlp_outputs_and_posns_to_keep = {}\n",
        "    batch, seq = len(means_dataset), means_dataset.max_len\n",
        "\n",
        "    for layer in range(model.cfg.n_layers):\n",
        "        mask = t.zeros(size=(batch, seq))\n",
        "\n",
        "        for (mlp_type, layer_list) in circuit.items():\n",
        "            seq_pos = seq_pos_to_keep[mlp_type]\n",
        "            indices = means_dataset.word_idx[seq_pos]\n",
        "            if layer in layer_list:  # Check if the current layer is in the layer list for this mlp_type\n",
        "                mask[:, indices] = 1\n",
        "\n",
        "        mlp_outputs_and_posns_to_keep[layer] = mask.bool()\n",
        "\n",
        "    return mlp_outputs_and_posns_to_keep"
      ],
      "metadata": {
        "id": "MH4KI_wCu7M-"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_outputs_and_posns_to_keep = get_mlp_outputs_and_posns_to_keep(dataset_2, model, CIRCUIT, SEQ_POS_TO_KEEP)"
      ],
      "metadata": {
        "id": "TU9VwBwquaIj"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hook_fn_mask_mlp_out(\n",
        "    mlp_out: Float[Tensor, \"batch seq d_mlp\"],\n",
        "    hook: HookPoint,\n",
        "    mlp_outputs_and_posns_to_keep: Dict[int, Bool[Tensor, \"batch seq\"]],\n",
        "    means: Float[Tensor, \"layer batch seq d_mlp\"],\n",
        ") -> Float[Tensor, \"batch seq d_mlp\"]:\n",
        "    '''\n",
        "    Hook function which masks the MLP output of a transformer layer.\n",
        "\n",
        "    mlp_outputs_and_posns_to_keep\n",
        "        Dict created with the get_mlp_outputs_and_posns_to_keep function. This tells\n",
        "        us where to mask.\n",
        "\n",
        "    means\n",
        "        Tensor of mean MLP output values of the means_dataset over each group of prompts\n",
        "        with the same template. This tells us what values to mask with.\n",
        "    '''\n",
        "    # Get the mask for this layer, adapted for MLP output structure\n",
        "    mask_for_this_layer = mlp_outputs_and_posns_to_keep[hook.layer()].unsqueeze(-1).to(mlp_out.device)\n",
        "\n",
        "    # Set MLP output values to the mean where necessary\n",
        "    mlp_out = t.where(mask_for_this_layer, mlp_out, means[hook.layer()])\n",
        "\n",
        "    return mlp_out"
      ],
      "metadata": {
        "id": "fXWq7V0Mv0F4"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def add_mean_ablation_hook(\n",
        "#     model: HookedTransformer,\n",
        "#     means_dataset: Dataset,\n",
        "#     circuit: Dict[str, List[Tuple[int, int]]] = CIRCUIT,\n",
        "#     seq_pos_to_keep: Dict[str, str] = SEQ_POS_TO_KEEP,\n",
        "#     is_permanent: bool = True,\n",
        "# ) -> HookedTransformer:\n",
        "#     '''\n",
        "#     Adds a permanent hook to the model, which ablates according to the circuit and\n",
        "#     seq_pos_to_keep dictionaries.\n",
        "\n",
        "#     In other words, when the model is run on ioi_dataset, every head's output will\n",
        "#     be replaced with the mean over means_dataset for sequences with the same template,\n",
        "#     except for a subset of heads and sequence positions as specified by the circuit\n",
        "#     and seq_pos_to_keep dicts.\n",
        "#     '''\n",
        "\n",
        "#     model.reset_hooks(including_permanent=True)\n",
        "\n",
        "#     # Compute the mean of each head's output on the ABC dataset, grouped by template\n",
        "#     means = compute_means_by_template(means_dataset, model)\n",
        "\n",
        "#     # Convert this into a boolean map\n",
        "#     heads_and_posns_to_keep = get_heads_and_posns_to_keep(means_dataset, model, circuit, seq_pos_to_keep)\n",
        "\n",
        "#     # Get a hook function which will patch in the mean z values for each head, at\n",
        "#     # all positions which aren't important for the circuit\n",
        "#     hook_fn = partial(\n",
        "#         hook_fn_mask_z,\n",
        "#         heads_and_posns_to_keep=heads_and_posns_to_keep,\n",
        "#         means=means\n",
        "#     )\n",
        "\n",
        "#     # Apply hook\n",
        "#     model.add_hook(lambda name: name.endswith(\"z\"), hook_fn, is_permanent=is_permanent)\n",
        "\n",
        "#     return model"
      ],
      "metadata": {
        "id": "JDKKsMBfWcAl"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.reset_hooks(including_permanent=True)\n",
        "logits_orig, orig_cache = model.run_with_cache(dataset.toks)\n",
        "orig_score = logits_to_ave_logit_diff(logits_orig, dataset)"
      ],
      "metadata": {
        "id": "oug8qZm81f-B"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits_orig.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLs_RFV19luW",
        "outputId": "437ccc99-fd82-41f6-d41d-9817749c9f00"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 4, 50257])"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n",
        "\n",
        "# Get a hook function which will patch in the mean MLP output values for each MLP layer, at\n",
        "# all positions which aren't important for the circuit\n",
        "hook_fn = partial(\n",
        "    hook_fn_mask_mlp_out,\n",
        "    mlp_outputs_and_posns_to_keep = mlp_outputs_and_posns_to_keep,\n",
        "    means=means\n",
        ")\n",
        "\n",
        "# Apply hook\n",
        "model.add_hook(lambda name: name.endswith(\"mlp_out\"), hook_fn, is_permanent=True)\n",
        "\n",
        "new_logits = model(dataset.toks)\n",
        "# new_logits.size()\n",
        "\n",
        "new_score = logits_to_ave_logit_diff(new_logits, dataset)\n",
        "\n",
        "print(f\"Average logit difference (dataset, using entire model): {orig_score:.4f}\")\n",
        "print(f\"Average logit difference (dataset, only using circuit): {new_score:.4f}\")\n",
        "print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ko9b8NYPwIlv",
        "outputId": "4c5e49cf-387f-4d2a-c850-21614ccdc0f0"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average logit difference (dataset, using entire model): 5.8226\n",
            "Average logit difference (dataset, only using circuit): 5.8226\n",
            "Average logit difference (circuit / full) %: 100.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_mean_ablation_hook_MLP(\n",
        "    model: HookedTransformer,\n",
        "    means_dataset: Dataset,\n",
        "    circuit: Dict[str, List[Tuple[int, int]]] = CIRCUIT,\n",
        "    seq_pos_to_keep: Dict[str, str] = SEQ_POS_TO_KEEP,\n",
        "    is_permanent: bool = True,\n",
        ") -> HookedTransformer:\n",
        "    '''\n",
        "    Adds a permanent hook to the model, which ablates according to the circuit and\n",
        "    seq_pos_to_keep dictionaries.\n",
        "\n",
        "    In other words, when the model is run on ioi_dataset, every head's output will\n",
        "    be replaced with the mean over means_dataset for sequences with the same template,\n",
        "    except for a subset of heads and sequence positions as specified by the circuit\n",
        "    and seq_pos_to_keep dicts.\n",
        "    '''\n",
        "\n",
        "    model.reset_hooks(including_permanent=True)\n",
        "\n",
        "    # Compute the mean of each head's output on the ABC dataset, grouped by template\n",
        "    means = compute_means_by_template_MLP(means_dataset, model)\n",
        "\n",
        "    # Convert this into a boolean map\n",
        "    mlp_outputs_and_posns_to_keep = get_mlp_outputs_and_posns_to_keep(means_dataset, model, circuit, seq_pos_to_keep)\n",
        "\n",
        "    # Get a hook function which will patch in the mean z values for each head, at\n",
        "    # all positions which aren't important for the circuit\n",
        "    hook_fn = partial(\n",
        "        hook_fn_mask_mlp_out,\n",
        "        mlp_outputs_and_posns_to_keep=mlp_outputs_and_posns_to_keep,\n",
        "        means=means\n",
        "    )\n",
        "\n",
        "    # Apply hook\n",
        "    model.add_hook(lambda name: name.endswith(\"mlp_out\"), hook_fn, is_permanent=True)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "sJlawX18v-yD"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_ablate_by_lst_MLP(lst, model, orig_score, print_output=True):\n",
        "    CIRCUIT = {\n",
        "        \"number mover\": lst,\n",
        "        # \"number mover 4\": lst,\n",
        "        \"number mover 3\": lst,\n",
        "        \"number mover 2\": lst,\n",
        "        \"number mover 1\": lst,\n",
        "    }\n",
        "\n",
        "    SEQ_POS_TO_KEEP = {\n",
        "        \"number mover\": \"end\",\n",
        "        # \"number mover 4\": \"S4\",\n",
        "        \"number mover 3\": \"S3\",\n",
        "        \"number mover 2\": \"S2\",\n",
        "        \"number mover 1\": \"S1\",\n",
        "    }\n",
        "\n",
        "    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n",
        "\n",
        "    # ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n",
        "\n",
        "    model = add_mean_ablation_hook_MLP(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n",
        "    new_logits = model(dataset.toks)\n",
        "\n",
        "    # orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n",
        "    new_score = logits_to_ave_logit_diff(new_logits, dataset)\n",
        "    if print_output:\n",
        "        # print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n",
        "        # print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n",
        "        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n",
        "    # return new_score\n",
        "    return 100 * new_score / orig_score"
      ],
      "metadata": {
        "id": "Ko6itvH15NtO"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lst = [layer for layer in range(12) if layer !=9]\n",
        "perc_of_orig = mean_ablate_by_lst_MLP(lst, model, orig_score, print_output=False).item()\n",
        "perc_of_orig"
      ],
      "metadata": {
        "id": "NDt2mk245r94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(12):\n",
        "    lst = [layer for layer in range(12) if layer != i]\n",
        "    perc_of_orig = mean_ablate_by_lst_MLP(lst, model, orig_score, print_output=False).item()\n",
        "    print(i, perc_of_orig)"
      ],
      "metadata": {
        "id": "I9SR5ETh6BWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_to_not_ablate = [0, 1, 2, 4, 6, 9, 10, 11]\n",
        "perc_of_orig = mean_ablate_by_lst_MLP(mlp_to_not_ablate, model, orig_score, print_output=False).item()\n",
        "print(perc_of_orig)"
      ],
      "metadata": {
        "id": "kszdCRW_6cmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_to_not_ablate = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "perc_of_orig = mean_ablate_by_lst_MLP(mlp_to_not_ablate, model, orig_score, print_output=False).item()\n",
        "print(perc_of_orig)"
      ],
      "metadata": {
        "id": "LkydYsX97OUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_to_not_ablate = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "perc_of_orig = mean_ablate_by_lst_MLP(mlp_to_not_ablate, model, orig_score, print_output=False).item()\n",
        "print(perc_of_orig)"
      ],
      "metadata": {
        "id": "Vg_k9Vfw7vY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_to_not_ablate = [0, 8, 9, 10, 11]\n",
        "perc_of_orig = mean_ablate_by_lst_MLP(mlp_to_not_ablate, model, orig_score, print_output=False).item()\n",
        "print(perc_of_orig)"
      ],
      "metadata": {
        "id": "1ZTAj7twGVrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ablate model and run"
      ],
      "metadata": {
        "id": "WWceCS9oY6PZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_prompt(\n",
        "    prompt: str,\n",
        "    answer: str,\n",
        "    model,  # Can't give type hint due to circular imports\n",
        "    prepend_space_to_answer: Optional[bool] = True,\n",
        "    print_details: Optional[bool] = True,\n",
        "    prepend_bos: Optional[bool] = USE_DEFAULT_VALUE,\n",
        "    top_k: Optional[int] = 10,\n",
        ") -> None:\n",
        "    \"\"\"Test if the Model Can Give the Correct Answer to a Prompt.\n",
        "\n",
        "    Intended for exploratory analysis. Prints out the performance on the answer (rank, logit, prob),\n",
        "    as well as the top k tokens. Works for multi-token prompts and multi-token answers.\n",
        "\n",
        "    Warning:\n",
        "\n",
        "    This will print the results (it does not return them).\n",
        "\n",
        "    Examples:\n",
        "\n",
        "    >>> from transformer_lens import HookedTransformer, utils\n",
        "    >>> model = HookedTransformer.from_pretrained(\"tiny-stories-1M\")\n",
        "    Loaded pretrained model tiny-stories-1M into HookedTransformer\n",
        "\n",
        "    >>> prompt = \"Why did the elephant cross the\"\n",
        "    >>> answer = \"road\"\n",
        "    >>> utils.test_prompt(prompt, answer, model)\n",
        "    Tokenized prompt: ['<|endoftext|>', 'Why', ' did', ' the', ' elephant', ' cross', ' the']\n",
        "    Tokenized answer: [' road']\n",
        "    Performance on answer token:\n",
        "    Rank: 2        Logit: 14.24 Prob:  3.51% Token: | road|\n",
        "    Top 0th token. Logit: 14.51 Prob:  4.59% Token: | ground|\n",
        "    Top 1th token. Logit: 14.41 Prob:  4.18% Token: | tree|\n",
        "    Top 2th token. Logit: 14.24 Prob:  3.51% Token: | road|\n",
        "    Top 3th token. Logit: 14.22 Prob:  3.45% Token: | car|\n",
        "    Top 4th token. Logit: 13.92 Prob:  2.55% Token: | river|\n",
        "    Top 5th token. Logit: 13.79 Prob:  2.25% Token: | street|\n",
        "    Top 6th token. Logit: 13.77 Prob:  2.21% Token: | k|\n",
        "    Top 7th token. Logit: 13.75 Prob:  2.16% Token: | hill|\n",
        "    Top 8th token. Logit: 13.64 Prob:  1.92% Token: | swing|\n",
        "    Top 9th token. Logit: 13.46 Prob:  1.61% Token: | park|\n",
        "    Ranks of the answer tokens: [(' road', 2)]\n",
        "\n",
        "    Args:\n",
        "        prompt:\n",
        "            The prompt string, e.g. \"Why did the elephant cross the\".\n",
        "        answer:\n",
        "            The answer, e.g. \"road\". Note that if you set prepend_space_to_answer to False, you need\n",
        "            to think about if you have a space before the answer here (as e.g. in this example the\n",
        "            answer may really be \" road\" if the prompt ends without a trailing space).\n",
        "        model:\n",
        "            The model.\n",
        "        prepend_space_to_answer:\n",
        "            Whether or not to prepend a space to the answer. Note this will only ever prepend a\n",
        "            space if the answer doesn't already start with one.\n",
        "        print_details:\n",
        "            Print the prompt (as a string but broken up by token), answer and top k tokens (all\n",
        "            with logit, rank and probability).\n",
        "        prepend_bos:\n",
        "            Overrides self.cfg.default_prepend_bos if set. Whether to prepend\n",
        "            the BOS token to the input (applicable when input is a string). Models generally learn\n",
        "            to use the BOS token as a resting place for attention heads (i.e. a way for them to be\n",
        "            \"turned off\"). This therefore often improves performance slightly.\n",
        "        top_k:\n",
        "            Top k tokens to print details of (when print_details is set to True).\n",
        "\n",
        "    Returns:\n",
        "        None (just prints the results directly).\n",
        "    \"\"\"\n",
        "    if prepend_space_to_answer and not answer.startswith(\" \"):\n",
        "        answer = \" \" + answer\n",
        "    # GPT-2 often treats the first token weirdly, so lets give it a resting position\n",
        "    prompt_tokens = model.to_tokens(prompt, prepend_bos=prepend_bos)\n",
        "    answer_tokens = model.to_tokens(answer, prepend_bos=False)\n",
        "    tokens = torch.cat((prompt_tokens, answer_tokens), dim=1)\n",
        "    prompt_str_tokens = model.to_str_tokens(prompt, prepend_bos=prepend_bos)\n",
        "    answer_str_tokens = model.to_str_tokens(answer, prepend_bos=False)\n",
        "    prompt_length = len(prompt_str_tokens)\n",
        "    answer_length = len(answer_str_tokens)\n",
        "    if print_details:\n",
        "        print(\"Tokenized prompt:\", prompt_str_tokens)\n",
        "        print(\"Tokenized answer:\", answer_str_tokens)\n",
        "    logits = remove_batch_dim(model(tokens))\n",
        "    probs = logits.softmax(dim=-1)\n",
        "    answer_ranks = []\n",
        "    for index in range(prompt_length, prompt_length + answer_length):\n",
        "        answer_token = tokens[0, index]\n",
        "        answer_str_token = answer_str_tokens[index - prompt_length]\n",
        "        # Offset by 1 because models predict the NEXT token\n",
        "        token_probs = probs[index - 1]\n",
        "        sorted_token_probs, sorted_token_values = token_probs.sort(descending=True)\n",
        "        # Janky way to get the index of the token in the sorted list - I couldn't find a better way?\n",
        "        correct_rank = torch.arange(len(sorted_token_values))[\n",
        "            (sorted_token_values == answer_token).cpu()\n",
        "        ].item()\n",
        "        answer_ranks.append((answer_str_token, correct_rank))\n",
        "        if print_details:\n",
        "            # String formatting syntax - the first number gives the number of characters to pad to, the second number gives the number of decimal places.\n",
        "            # rprint gives rich text printing\n",
        "            rprint(\n",
        "                f\"Performance on answer token:\\n[b]Rank: {correct_rank: <8} Logit: {logits[index-1, answer_token].item():5.2f} Prob: {token_probs[answer_token].item():6.2%} Token: |{answer_str_token}|[/b]\"\n",
        "            )\n",
        "            for i in range(top_k):\n",
        "                print(\n",
        "                    f\"Top {i}th token. Logit: {logits[index-1, sorted_token_values[i]].item():5.2f} Prob: {sorted_token_probs[i].item():6.2%} Token: |{model.to_string(sorted_token_values[i])}|\"\n",
        "                )\n",
        "    rprint(f\"[b]Ranks of the answer tokens:[/b] {answer_ranks}\")"
      ],
      "metadata": {
        "id": "6OfqM2mkaT5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_prompt(\n",
        "    prompt = \"1 2 3 4\",\n",
        "    answer: \" 5\",\n",
        "    model,\n",
        "    prepend_space_to_answer= True,\n",
        "    print_details = True,\n",
        "    prepend_bos = True,\n",
        "    top_k = 10,\n",
        ")"
      ],
      "metadata": {
        "id": "htAmfQQQaWlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n"
      ],
      "metadata": {
        "id": "rclHQHT54SFR"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hook operates on the seq len of the means dataset (which has 4 tokens). Thus, don't prepend or add the answer to it, else that's two extra tokens!"
      ],
      "metadata": {
        "id": "Fi7s-Sax5ms1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if prepend_space_to_answer and not answer.startswith(\" \"):\n",
        "#     answer = \" \" + answer\n",
        "# GPT-2 often treats the first token weirdly, so lets give it a resting position\n",
        "prompt_tokens = model.to_tokens(prompt, prepend_bos=False)\n",
        "answer_tokens = model.to_tokens(answer, prepend_bos=False)\n",
        "# tokens = torch.cat((prompt_tokens, answer_tokens), dim=1)\n",
        "prompt_str_tokens = model.to_str_tokens(prompt, prepend_bos=False)\n",
        "answer_str_tokens = model.to_str_tokens(answer, prepend_bos=False)\n",
        "prompt_length = len(prompt_str_tokens)\n",
        "answer_length = len(answer_str_tokens)\n",
        "\n",
        "print(\"Tokenized prompt:\", prompt_str_tokens)\n",
        "print(\"Tokenized answer:\", answer_str_tokens)\n",
        "logits = remove_batch_dim(model(prompt_tokens))\n",
        "probs = logits.softmax(dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwCEVPQq4tOn",
        "outputId": "74ad45d3-647b-4732-d16f-e60666c19fb4"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized prompt: ['1', ' 2', ' 3', ' 4']\n",
            "Tokenized answer: [' 5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_token_values == answer_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ypn6oFSa6XSo",
        "outputId": "a5d79a36-6f9c-4ecd-dafb-3f626150fb3b"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ True, False, False,  ..., False, False, False])"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_token_values.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJ5xZLUP7DlO",
        "outputId": "29212af6-2e71-4a9c-9a5e-3fa313de16bb"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50257])"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.arange(len(sorted_token_values))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvqryAbE6rN_",
        "outputId": "a14c8b23-5546-46c8-9cec-874c5082ac69"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 6\n",
        "\n",
        "answer_ranks = []\n",
        "for index in range(prompt_length, prompt_length + answer_length):\n",
        "    answer_token = tokens[0, index]\n",
        "    answer_str_token = answer_str_tokens[index - prompt_length]\n",
        "    # Offset by 1 because models predict the NEXT token\n",
        "    token_probs = probs[index - 1]\n",
        "    sorted_token_probs, sorted_token_values = token_probs.sort(descending=True) # sorted_token_values are the indices; the indices correspond to token num of vocab\n",
        "    # Janky way to get the index of the token in the sorted list - I couldn't find a better way?\n",
        "    correct_rank = torch.arange(len(sorted_token_values))[\n",
        "        (sorted_token_values == answer_token).cpu()\n",
        "    ].item()\n",
        "    answer_ranks.append((answer_str_token, correct_rank))\n",
        "\n",
        "    # String formatting syntax - the first number gives the number of characters to pad to, the second number gives the number of decimal places.\n",
        "    # rprint gives rich text printing\n",
        "    print(\n",
        "        f\"Performance on answer token:\\n[b]Rank: {correct_rank: <8} Logit: {logits[index-1, answer_token].item():5.2f} Prob: {token_probs[answer_token].item():6.2%} Token: |{answer_str_token}|[/b]\"\n",
        "    )\n",
        "    for i in range(top_k):\n",
        "        print(\n",
        "            f\"Top {i}th token. Logit: {logits[index-1, sorted_token_values[i]].item():5.2f} Prob: {sorted_token_probs[i].item():6.2%} Token: |{model.to_string(sorted_token_values[i])}|\"\n",
        "        )\n",
        "print(f\"[b]Ranks of the answer tokens:[/b] {answer_ranks}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNJW2mxx5jT-",
        "outputId": "aed60501-92d3-4a45-e5e6-1d698390992b"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance on answer token:\n",
            "[b]Rank: 0        Logit: 16.81 Prob: 92.39% Token: | 5|[/b]\n",
            "Top 0th token. Logit: 16.81 Prob: 92.39% Token: | 5|\n",
            "Top 1th token. Logit: 11.62 Prob:  0.51% Token: | 4|\n",
            "Top 2th token. Logit: 11.34 Prob:  0.39% Token: | 1|\n",
            "Top 3th token. Logit: 11.23 Prob:  0.35% Token: | 6|\n",
            "Top 4th token. Logit: 11.10 Prob:  0.30% Token: | 3|\n",
            "Top 5th token. Logit: 10.88 Prob:  0.24% Token: | 0|\n",
            "[b]Ranks of the answer tokens:[/b] [(' 5', 0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n",
        "\n",
        "# Get a hook function which will patch in the mean MLP output values for each MLP layer, at\n",
        "# all positions which aren't important for the circuit\n",
        "hook_fn = partial(\n",
        "    hook_fn_mask_mlp_out,\n",
        "    mlp_outputs_and_posns_to_keep = mlp_outputs_and_posns_to_keep,\n",
        "    means=means\n",
        ")\n",
        "\n",
        "# Apply hook\n",
        "model.add_hook(lambda name: name.endswith(\"mlp_out\"), hook_fn, is_permanent=True)\n",
        "\n",
        "# new_logits = model(dataset.toks)\n",
        "# # new_logits.size()\n",
        "\n",
        "# new_score = logits_to_ave_logit_diff(new_logits, dataset)\n",
        "\n",
        "# print(f\"Average logit difference (dataset, using entire model): {orig_score:.4f}\")\n",
        "# print(f\"Average logit difference (dataset, only using circuit): {new_score:.4f}\")\n",
        "# print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")"
      ],
      "metadata": {
        "id": "VXnloXIP7Lm8"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_prompt = \"1 2 3 4\"\n",
        "example_answer = \" 5\"\n",
        "utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "outputId": "2cf2e0ac-3a32-424b-8c43-994a770c2efd",
        "id": "UD4YqZY17Lm9"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized prompt: ['<|endoftext|>', '1', ' 2', ' 3', ' 4']\n",
            "Tokenized answer: [' 5']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-111-5361d296e9ed>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexample_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"1 2 3 4\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mexample_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" 5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_answer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend_bos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/utils.py\u001b[0m in \u001b[0;36mtest_prompt\u001b[0;34m(prompt, answer, model, prepend_space_to_answer, print_details, prepend_bos, top_k)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tokenized prompt:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt_str_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tokenized answer:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_str_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_batch_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0manswer_ranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/HookedTransformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    553\u001b[0m                     )\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m                 residual = block(\n\u001b[0m\u001b[1;32m    556\u001b[0m                     \u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m                     \u001b[0;31m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/components.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m   1207\u001b[0m             )\n\u001b[1;32m   1208\u001b[0m             \u001b[0mnormalized_resid_mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m             mlp_out = self.hook_mlp_out(\n\u001b[0m\u001b[1;32m   1210\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized_resid_mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m             )  # [batch, pos, d_model]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m                         \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1581\u001b[0;31m                         \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1583\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mhook_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/hook_points.py\u001b[0m in \u001b[0;36mfull_hook\u001b[0;34m(module, module_input, module_output)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfull_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             full_hook.__name__ = (\n",
            "\u001b[0;32m<ipython-input-65-f99588862349>\u001b[0m in \u001b[0;36mhook_fn_mask_mlp_out\u001b[0;34m(mlp_out, hook, mlp_outputs_and_posns_to_keep, means)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Set MLP output values to the mean where necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mmlp_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_for_this_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlp_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmlp_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (6) at non-singleton dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hook operates on the seq len of the means dataset (which has 4 tokens). Thus, don't prepend or add the answer to it, else that's two extra tokens!"
      ],
      "metadata": {
        "id": "minv9O9k7Lm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if prepend_space_to_answer and not answer.startswith(\" \"):\n",
        "#     answer = \" \" + answer\n",
        "# GPT-2 often treats the first token weirdly, so lets give it a resting position\n",
        "prompt_tokens = model.to_tokens(prompt, prepend_bos=False)\n",
        "answer_tokens = model.to_tokens(answer, prepend_bos=False)\n",
        "# tokens = torch.cat((prompt_tokens, answer_tokens), dim=1)\n",
        "prompt_str_tokens = model.to_str_tokens(prompt, prepend_bos=False)\n",
        "answer_str_tokens = model.to_str_tokens(answer, prepend_bos=False)\n",
        "prompt_length = len(prompt_str_tokens)\n",
        "answer_length = len(answer_str_tokens)\n",
        "\n",
        "print(\"Tokenized prompt:\", prompt_str_tokens)\n",
        "print(\"Tokenized answer:\", answer_str_tokens)\n",
        "logits = remove_batch_dim(model(prompt_tokens))\n",
        "probs = logits.softmax(dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10b8cb9c-1e0e-4ea5-8d80-5fcc9c546405",
        "id": "-ektM8N67Lm9"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized prompt: ['1', ' 2', ' 3', ' 4']\n",
            "Tokenized answer: [' 5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model(prompt_tokens).size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GG5ENwkS8DEB",
        "outputId": "d432c77f-28b1-4fc7-8c94-ccce2243307a"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 4, 50257])"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0PwB3Jv7xUj",
        "outputId": "0b5c6e15-1898-4b6d-a3a7-8d9972700633"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 4, 50257])"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The issue here is that means dataset has batche size 100, so logits has 100. But why does this only happen after adding the hook?"
      ],
      "metadata": {
        "id": "jvc9pR5q70wo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 6\n",
        "\n",
        "answer_ranks = []\n",
        "for index in range(prompt_length, prompt_length + answer_length):\n",
        "    answer_token = tokens[0, index]\n",
        "    answer_str_token = answer_str_tokens[index - prompt_length]\n",
        "    # Offset by 1 because models predict the NEXT token\n",
        "    token_probs = probs[index - 1]\n",
        "    sorted_token_probs, sorted_token_values = token_probs.sort(descending=True) # sorted_token_values are the indices; the indices correspond to token num of vocab\n",
        "    break\n",
        "sorted_token_values == answer_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f79c97b5-cce3-4f25-d1eb-59d6e2677fe6",
        "id": "f7ujGcN-7Lm9"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[False, False, False,  ..., False, False, False],\n",
              "        [False, False, False,  ..., False, False, False],\n",
              "        [False, False, False,  ..., False, False, False],\n",
              "        [ True, False, False,  ..., False, False, False]])"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probs.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sY8cgGWy7n68",
        "outputId": "699bc575-f2de-4f66-dd83-9bd0b69c7149"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 4, 50257])"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_token_values.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "991bdfd2-2a97-4758-9fab-505aace55d14",
        "id": "F2AE_8OR7Lm-"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 50257])"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.arange(len(sorted_token_values))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a14c8b23-5546-46c8-9cec-874c5082ac69",
        "id": "tqyJ6CMu7Lm-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 6\n",
        "\n",
        "answer_ranks = []\n",
        "for index in range(prompt_length, prompt_length + answer_length):\n",
        "    answer_token = tokens[0, index]\n",
        "    answer_str_token = answer_str_tokens[index - prompt_length]\n",
        "    # Offset by 1 because models predict the NEXT token\n",
        "    token_probs = probs[index - 1]\n",
        "    sorted_token_probs, sorted_token_values = token_probs.sort(descending=True) # sorted_token_values are the indices; the indices correspond to token num of vocab\n",
        "    # Janky way to get the index of the token in the sorted list - I couldn't find a better way?\n",
        "    correct_rank = torch.arange(len(sorted_token_values))[\n",
        "        (sorted_token_values == answer_token).cpu()\n",
        "    ].item()\n",
        "    answer_ranks.append((answer_str_token, correct_rank))\n",
        "\n",
        "    # String formatting syntax - the first number gives the number of characters to pad to, the second number gives the number of decimal places.\n",
        "    # rprint gives rich text printing\n",
        "    print(\n",
        "        f\"Performance on answer token:\\n[b]Rank: {correct_rank: <8} Logit: {logits[index-1, answer_token].item():5.2f} Prob: {token_probs[answer_token].item():6.2%} Token: |{answer_str_token}|[/b]\"\n",
        "    )\n",
        "    for i in range(top_k):\n",
        "        print(\n",
        "            f\"Top {i}th token. Logit: {logits[index-1, sorted_token_values[i]].item():5.2f} Prob: {sorted_token_probs[i].item():6.2%} Token: |{model.to_string(sorted_token_values[i])}|\"\n",
        "        )\n",
        "print(f\"[b]Ranks of the answer tokens:[/b] {answer_ranks}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aed60501-92d3-4a45-e5e6-1d698390992b",
        "id": "1qJPl5a57Lm-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance on answer token:\n",
            "[b]Rank: 0        Logit: 16.81 Prob: 92.39% Token: | 5|[/b]\n",
            "Top 0th token. Logit: 16.81 Prob: 92.39% Token: | 5|\n",
            "Top 1th token. Logit: 11.62 Prob:  0.51% Token: | 4|\n",
            "Top 2th token. Logit: 11.34 Prob:  0.39% Token: | 1|\n",
            "Top 3th token. Logit: 11.23 Prob:  0.35% Token: | 6|\n",
            "Top 4th token. Logit: 11.10 Prob:  0.30% Token: | 3|\n",
            "Top 5th token. Logit: 10.88 Prob:  0.24% Token: | 0|\n",
            "[b]Ranks of the answer tokens:[/b] [(' 5', 0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## try dataset means of size 1"
      ],
      "metadata": {
        "id": "Jbir4u-A9IYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_2.toks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIxG6s2I-Bxf",
        "outputId": "694e1ba2-9a48-4b13-af2b-daa5fc22ec81"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[5999, 8684, 6073, 1802]], dtype=torch.int32)"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "means = compute_means_by_template_MLP(dataset_2, model)\n",
        "means.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irYDBZN--Mnm",
        "outputId": "6355924a-2166-4200-fa0a-51d631da821b"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12, 1, 4, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heavily depends on means to get logit size"
      ],
      "metadata": {
        "id": "F2hyrMSX-O27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lst = list(range(9))\n",
        "\n",
        "CIRCUIT = {\n",
        "    \"number mover\": lst,\n",
        "    # \"number mover 4\": lst,\n",
        "    \"number mover 3\": lst,\n",
        "    \"number mover 2\": lst,\n",
        "    \"number mover 1\": lst,\n",
        "}\n",
        "\n",
        "SEQ_POS_TO_KEEP = {\n",
        "    \"number mover\": \"end\",\n",
        "    # \"number mover 4\": \"S4\",\n",
        "    \"number mover 3\": \"S3\",\n",
        "    \"number mover 2\": \"S2\",\n",
        "    \"number mover 1\": \"S1\",\n",
        "}"
      ],
      "metadata": {
        "id": "V06TRL2P_Iq2"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_outputs_and_posns_to_keep = get_mlp_outputs_and_posns_to_keep(dataset_2, model, CIRCUIT, SEQ_POS_TO_KEEP)"
      ],
      "metadata": {
        "id": "KSS9puW29Trb"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n",
        "\n",
        "# Get a hook function which will patch in the mean MLP output values for each MLP layer, at\n",
        "# all positions which aren't important for the circuit\n",
        "hook_fn = partial(\n",
        "    hook_fn_mask_mlp_out,\n",
        "    mlp_outputs_and_posns_to_keep = mlp_outputs_and_posns_to_keep,\n",
        "    means=means\n",
        ")\n",
        "\n",
        "# Apply hook\n",
        "model.add_hook(lambda name: name.endswith(\"mlp_out\"), hook_fn, is_permanent=True)\n",
        "\n",
        "# new_logits = model(dataset.toks)\n",
        "# # new_logits.size()\n",
        "\n",
        "# new_score = logits_to_ave_logit_diff(new_logits, dataset)\n",
        "\n",
        "# print(f\"Average logit difference (dataset, using entire model): {orig_score:.4f}\")\n",
        "# print(f\"Average logit difference (dataset, only using circuit): {new_score:.4f}\")\n",
        "# print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")"
      ],
      "metadata": {
        "id": "ZIrHf_3L9KJ1"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if prepend_space_to_answer and not answer.startswith(\" \"):\n",
        "#     answer = \" \" + answer\n",
        "# GPT-2 often treats the first token weirdly, so lets give it a resting position\n",
        "prompt_tokens = model.to_tokens(prompt, prepend_bos=False)\n",
        "answer_tokens = model.to_tokens(answer, prepend_bos=False)\n",
        "# tokens = torch.cat((prompt_tokens, answer_tokens), dim=1)\n",
        "prompt_str_tokens = model.to_str_tokens(prompt, prepend_bos=False)\n",
        "answer_str_tokens = model.to_str_tokens(answer, prepend_bos=False)\n",
        "prompt_length = len(prompt_str_tokens)\n",
        "answer_length = len(answer_str_tokens)\n",
        "\n",
        "print(\"Tokenized prompt:\", prompt_str_tokens)\n",
        "print(\"Tokenized answer:\", answer_str_tokens)\n",
        "logits = remove_batch_dim(model(prompt_tokens))\n",
        "probs = logits.softmax(dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc9bdff5-6768-425f-a805-2c60d361035f",
        "id": "8ZyBy4dg9KJ8"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized prompt: ['1', ' 2', ' 3', ' 4']\n",
            "Tokenized answer: [' 5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model(prompt_tokens).size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bc8d062-a310-4adf-9556-07812969390c",
        "id": "kpngP9ls9KJ8"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 50257])"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "541cc79c-9502-455e-d21e-688227510c04",
        "id": "QJhYGil79KJ8"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 50257])"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The issue here is that means dataset has batche size 100, so logits has 100. But why does this only happen after adding the hook?"
      ],
      "metadata": {
        "id": "nSVY-bmp9KJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 6\n",
        "\n",
        "answer_ranks = []\n",
        "for index in range(prompt_length, prompt_length + answer_length):\n",
        "    answer_token = tokens[0, index]\n",
        "    answer_str_token = answer_str_tokens[index - prompt_length]\n",
        "    # Offset by 1 because models predict the NEXT token\n",
        "    token_probs = probs[index - 1]\n",
        "    sorted_token_probs, sorted_token_values = token_probs.sort(descending=True) # sorted_token_values are the indices; the indices correspond to token num of vocab\n",
        "    break\n",
        "sorted_token_values == answer_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e353e23-056f-4af6-9739-59b9477b62e2",
        "id": "TRnHMtRo9KJ8"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([False,  True, False,  ..., False, False, False])"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probs.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cc57af1-d0fe-4bda-81fd-568afff4944b",
        "id": "11hylh0q9KJ8"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 50257])"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_token_values.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18e03947-8764-4d76-a41e-322ce095e53b",
        "id": "33hLFk9m9KJ8"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50257])"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.arange(len(sorted_token_values))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fb89de5-7fd7-4c95-be84-d97d2bb30024",
        "id": "CTAnW9ek9KJ8"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([    0,     1,     2,  ..., 50254, 50255, 50256])"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 6\n",
        "\n",
        "answer_ranks = []\n",
        "for index in range(prompt_length, prompt_length + answer_length):\n",
        "    answer_token = tokens[0, index]\n",
        "    answer_str_token = answer_str_tokens[index - prompt_length]\n",
        "    # Offset by 1 because models predict the NEXT token\n",
        "    token_probs = probs[index - 1]\n",
        "    sorted_token_probs, sorted_token_values = token_probs.sort(descending=True) # sorted_token_values are the indices; the indices correspond to token num of vocab\n",
        "    # Janky way to get the index of the token in the sorted list - I couldn't find a better way?\n",
        "    correct_rank = torch.arange(len(sorted_token_values))[\n",
        "        (sorted_token_values == answer_token).cpu()\n",
        "    ].item()\n",
        "    answer_ranks.append((answer_str_token, correct_rank))\n",
        "\n",
        "    # String formatting syntax - the first number gives the number of characters to pad to, the second number gives the number of decimal places.\n",
        "    # rprint gives rich text printing\n",
        "    print(\n",
        "        f\"Performance on answer token:\\n[b]Rank: {correct_rank: <8} Logit: {logits[index-1, answer_token].item():5.2f} Prob: {token_probs[answer_token].item():6.2%} Token: |{answer_str_token}|[/b]\"\n",
        "    )\n",
        "    for i in range(top_k):\n",
        "        print(\n",
        "            f\"Top {i}th token. Logit: {logits[index-1, sorted_token_values[i]].item():5.2f} Prob: {sorted_token_probs[i].item():6.2%} Token: |{model.to_string(sorted_token_values[i])}|\"\n",
        "        )\n",
        "print(f\"[b]Ranks of the answer tokens:[/b] {answer_ranks}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3089c7b3-9a23-45c0-dd0c-4c9616820280",
        "id": "wdyEmWcf9KJ8"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance on answer token:\n",
            "[b]Rank: 1        Logit: 14.38 Prob: 16.97% Token: | 5|[/b]\n",
            "Top 0th token. Logit: 14.86 Prob: 27.48% Token: | 4|\n",
            "Top 1th token. Logit: 14.38 Prob: 16.97% Token: | 5|\n",
            "Top 2th token. Logit: 13.88 Prob: 10.32% Token: | 3|\n",
            "Top 3th token. Logit: 13.54 Prob:  7.37% Token: | 6|\n",
            "Top 4th token. Logit: 13.54 Prob:  7.34% Token: | 1|\n",
            "Top 5th token. Logit: 13.08 Prob:  4.62% Token: | 2|\n",
            "[b]Ranks of the answer tokens:[/b] [(' 5', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that MLP ablation of MLPs 10 and 11 messes it up so that it chooses 4, but doesn't choose 5."
      ],
      "metadata": {
        "id": "ly8UPUl-_Tdw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP and Head together"
      ],
      "metadata": {
        "id": "DlpH0Wib-v1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_heads_and_posns_to_keep(\n",
        "    means_dataset: Dataset,\n",
        "    model: HookedTransformer,\n",
        "    circuit: Dict[str, List[Tuple[int, int]]],\n",
        "    seq_pos_to_keep: Dict[str, str],\n",
        ") -> Dict[int, Bool[Tensor, \"batch seq head\"]]:\n",
        "    '''\n",
        "    Returns a dictionary mapping layers to a boolean mask giving the indices of the\n",
        "    z output which *shouldn't* be mean-ablated.\n",
        "\n",
        "    The output of this function will be used for the hook function that does ablation.\n",
        "    '''\n",
        "    heads_and_posns_to_keep = {}\n",
        "    batch, seq, n_heads = len(means_dataset), means_dataset.max_len, model.cfg.n_heads\n",
        "\n",
        "    for layer in range(model.cfg.n_layers):\n",
        "\n",
        "        mask = t.zeros(size=(batch, seq, n_heads))\n",
        "\n",
        "        for (head_type, head_list) in circuit.items():\n",
        "            seq_pos = seq_pos_to_keep[head_type]\n",
        "            indices = means_dataset.word_idx[seq_pos] # modify this for key vs query pos. curr, this is query\n",
        "            for (layer_idx, head_idx) in head_list:\n",
        "                if layer_idx == layer:\n",
        "                    mask[:, indices, head_idx] = 1\n",
        "\n",
        "        heads_and_posns_to_keep[layer] = mask.bool()\n",
        "\n",
        "    return heads_and_posns_to_keep\n",
        "\n",
        "def hook_fn_mask_z(\n",
        "    z: Float[Tensor, \"batch seq head d_head\"],\n",
        "    hook: HookPoint,\n",
        "    heads_and_posns_to_keep: Dict[int, Bool[Tensor, \"batch seq head\"]],\n",
        "    means: Float[Tensor, \"layer batch seq head d_head\"],\n",
        ") -> Float[Tensor, \"batch seq head d_head\"]:\n",
        "    '''\n",
        "    Hook function which masks the z output of a transformer head.\n",
        "\n",
        "    heads_and_posns_to_keep\n",
        "        Dict created with the get_heads_and_posns_to_keep function. This tells\n",
        "        us where to mask.\n",
        "\n",
        "    means\n",
        "        Tensor of mean z values of the means_dataset over each group of prompts\n",
        "        with the same template. This tells us what values to mask with.\n",
        "    '''\n",
        "    # Get the mask for this layer, and add d_head=1 dimension so it broadcasts correctly\n",
        "    mask_for_this_layer = heads_and_posns_to_keep[hook.layer()].unsqueeze(-1).to(z.device)\n",
        "\n",
        "    # Set z values to the mean\n",
        "    z = t.where(mask_for_this_layer, z, means[hook.layer()])\n",
        "\n",
        "    return z\n",
        "\n",
        "def compute_means_by_template(\n",
        "    means_dataset: Dataset,\n",
        "    model: HookedTransformer\n",
        ") -> Float[Tensor, \"layer batch seq head_idx d_head\"]:\n",
        "    '''\n",
        "    Returns the mean of each head's output over the means dataset. This mean is\n",
        "    computed separately for each group of prompts with the same template (these\n",
        "    are given by means_dataset.groups).\n",
        "    '''\n",
        "    # Cache the outputs of every head\n",
        "    _, means_cache = model.run_with_cache(\n",
        "        means_dataset.toks.long(),\n",
        "        return_type=None,\n",
        "        names_filter=lambda name: name.endswith(\"z\"),\n",
        "    )\n",
        "    # Create tensor to store means\n",
        "    n_layers, n_heads, d_head = model.cfg.n_layers, model.cfg.n_heads, model.cfg.d_head\n",
        "    batch, seq_len = len(means_dataset), means_dataset.max_len\n",
        "    means = t.zeros(size=(n_layers, batch, seq_len, n_heads, d_head), device=model.cfg.device)\n",
        "\n",
        "    # Get set of different templates for this data\n",
        "    for layer in range(model.cfg.n_layers):\n",
        "        z_for_this_layer: Float[Tensor, \"batch seq head d_head\"] = means_cache[utils.get_act_name(\"z\", layer)]\n",
        "        for template_group in means_dataset.groups:\n",
        "            z_for_this_template = z_for_this_layer[template_group]\n",
        "            z_means_for_this_template = einops.reduce(z_for_this_template, \"batch seq head d_head -> seq head d_head\", \"mean\")\n",
        "            means[layer, template_group] = z_means_for_this_template\n",
        "\n",
        "    return means\n",
        "\n",
        "def add_mean_ablation_hook(\n",
        "    model: HookedTransformer,\n",
        "    means_dataset: Dataset,\n",
        "    circuit: Dict[str, List[Tuple[int, int]]] = CIRCUIT,\n",
        "    seq_pos_to_keep: Dict[str, str] = SEQ_POS_TO_KEEP,\n",
        "    is_permanent: bool = True,\n",
        ") -> HookedTransformer:\n",
        "    '''\n",
        "    Adds a permanent hook to the model, which ablates according to the circuit and\n",
        "    seq_pos_to_keep dictionaries.\n",
        "\n",
        "    In other words, when the model is run on ioi_dataset, every head's output will\n",
        "    be replaced with the mean over means_dataset for sequences with the same template,\n",
        "    except for a subset of heads and sequence positions as specified by the circuit\n",
        "    and seq_pos_to_keep dicts.\n",
        "    '''\n",
        "\n",
        "    model.reset_hooks(including_permanent=True)\n",
        "\n",
        "    # Compute the mean of each head's output on the ABC dataset, grouped by template\n",
        "    means = compute_means_by_template(means_dataset, model)\n",
        "\n",
        "    # Convert this into a boolean map\n",
        "    heads_and_posns_to_keep = get_heads_and_posns_to_keep(means_dataset, model, circuit, seq_pos_to_keep)\n",
        "\n",
        "    # Get a hook function which will patch in the mean z values for each head, at\n",
        "    # all positions which aren't important for the circuit\n",
        "    hook_fn = partial(\n",
        "        hook_fn_mask_z,\n",
        "        heads_and_posns_to_keep=heads_and_posns_to_keep,\n",
        "        means=means\n",
        "    )\n",
        "\n",
        "    # Apply hook\n",
        "    model.add_hook(lambda name: name.endswith(\"z\"), hook_fn, is_permanent=is_permanent)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "cE7xLtws_Pnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_mean_ablation_hook_MLP_head(\n",
        "    model: HookedTransformer,\n",
        "    means_dataset: Dataset,\n",
        "    heads_lst, mlp_lst,\n",
        "    is_permanent: bool = True,\n",
        ") -> HookedTransformer:\n",
        "    CIRCUIT = {\n",
        "        \"number mover\": heads_lst,\n",
        "        \"number mover 3\": heads_lst,\n",
        "        \"number mover 2\": heads_lst,\n",
        "        \"number mover 1\": heads_lst,\n",
        "    }\n",
        "\n",
        "    SEQ_POS_TO_KEEP = {\n",
        "        \"number mover\": \"end\",\n",
        "        \"number mover 3\": \"S3\",\n",
        "        \"number mover 2\": \"S2\",\n",
        "        \"number mover 1\": \"S1\",\n",
        "    }\n",
        "\n",
        "    model.reset_hooks(including_permanent=True)\n",
        "\n",
        "    # Compute the mean of each head's output on the ABC dataset, grouped by template\n",
        "    means = compute_means_by_template(means_dataset, model)\n",
        "\n",
        "    # Convert this into a boolean map\n",
        "    heads_and_posns_to_keep = get_heads_and_posns_to_keep(means_dataset, model, CIRCUIT, SEQ_POS_TO_KEEP)\n",
        "\n",
        "    # Get a hook function which will patch in the mean z values for each head, at\n",
        "    # all positions which aren't important for the circuit\n",
        "    hook_fn = partial(\n",
        "        hook_fn_mask_z,\n",
        "        heads_and_posns_to_keep=heads_and_posns_to_keep,\n",
        "        means=means\n",
        "    )\n",
        "\n",
        "    # Apply hook\n",
        "    model.add_hook(lambda name: name.endswith(\"z\"), hook_fn, is_permanent=is_permanent)\n",
        "\n",
        "    ########################\n",
        "    CIRCUIT = {\n",
        "        \"number mover\": mlp_lst,\n",
        "        \"number mover 3\": mlp_lst,\n",
        "        \"number mover 2\": mlp_lst,\n",
        "        \"number mover 1\": mlp_lst,\n",
        "    }\n",
        "\n",
        "    SEQ_POS_TO_KEEP = {\n",
        "        \"number mover\": \"end\",\n",
        "        \"number mover 3\": \"S3\",\n",
        "        \"number mover 2\": \"S2\",\n",
        "        \"number mover 1\": \"S1\",\n",
        "    }\n",
        "\n",
        "    # Compute the mean of each head's output on the ABC dataset, grouped by template\n",
        "    means = compute_means_by_template_MLP(means_dataset, model)\n",
        "\n",
        "    # Convert this into a boolean map\n",
        "    mlp_outputs_and_posns_to_keep = get_mlp_outputs_and_posns_to_keep(means_dataset, model, CIRCUIT, SEQ_POS_TO_KEEP)\n",
        "\n",
        "    # Get a hook function which will patch in the mean z values for each head, at\n",
        "    # all positions which aren't important for the circuit\n",
        "    hook_fn = partial(\n",
        "        hook_fn_mask_mlp_out,\n",
        "        mlp_outputs_and_posns_to_keep=mlp_outputs_and_posns_to_keep,\n",
        "        means=means\n",
        "    )\n",
        "\n",
        "    # Apply hook\n",
        "    model.add_hook(lambda name: name.endswith(\"mlp_out\"), hook_fn, is_permanent=True)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "mnGaDWe__CyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "heads_not_ablate = [(layer, head) for layer in range(12) for head in range(12)]\n",
        "mlps_not_ablate = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
        "\n",
        "model = add_mean_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n",
        "new_logits = model(dataset.toks)\n",
        "new_score = logits_to_ave_logit_diff(new_logits, dataset)\n",
        "print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfjSc0rEALmb",
        "outputId": "7e431e77-4b03-4dba-9972-8f4a6acc2980"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average logit difference (circuit / full) %: 100.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get rid of last layer\n",
        "\n",
        "heads_not_ablate = [(layer, head) for layer in range(11) for head in range(12)]\n",
        "mlps_not_ablate = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "\n",
        "model = add_mean_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n",
        "new_logits = model(dataset.toks)\n",
        "new_score = logits_to_ave_logit_diff(new_logits, dataset)\n",
        "print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHbM_oeSAsbO",
        "outputId": "2be3386d-d558-4d8c-8ed5-9c4c10946a4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average logit difference (circuit / full) %: 74.3485\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get rid of heads in L11\n",
        "\n",
        "heads_not_ablate = [(layer, head) for layer in range(11) for head in range(12)]\n",
        "mlps_not_ablate = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
        "\n",
        "model = add_mean_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n",
        "new_logits = model(dataset.toks)\n",
        "new_score = logits_to_ave_logit_diff(new_logits, dataset)\n",
        "print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ps_SUBDB4Zj",
        "outputId": "821ca894-7447-461b-a42f-62253a6b3556"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average logit difference (circuit / full) %: 99.3207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try ablating by orig dataset mean"
      ],
      "metadata": {
        "id": "XZnNkMQQCGDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get rid of heads in L11\n",
        "\n",
        "heads_not_ablate = [(layer, head) for layer in range(10) for head in range(10)]\n",
        "mlps_not_ablate = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "\n",
        "model = add_mean_ablation_hook_MLP_head(model, dataset, heads_not_ablate, mlps_not_ablate)\n",
        "new_logits = model(dataset.toks)\n",
        "new_score = logits_to_ave_logit_diff(new_logits, dataset)\n",
        "print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Qa7WXKJCJZx",
        "outputId": "626d15ab-35e2-486f-fb12-59b411179443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average logit difference (circuit / full) %: 30.9183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Head Ablation Expm Functions"
      ],
      "metadata": {
        "id": "BHHvz84w70vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model.reset_hooks(including_permanent=True)\n",
        "# ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n",
        "# orig_score = logits_to_ave_logit_diff(ioi_logits_original, dataset)"
      ],
      "metadata": {
        "id": "OI3FcmpMaNxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_ablate_by_lst(lst, model, orig_score, print_output=True):\n",
        "    CIRCUIT = {\n",
        "        \"number mover\": lst,\n",
        "        # \"number mover 4\": lst,\n",
        "        \"number mover 3\": lst,\n",
        "        \"number mover 2\": lst,\n",
        "        \"number mover 1\": lst,\n",
        "    }\n",
        "\n",
        "    SEQ_POS_TO_KEEP = {\n",
        "        \"number mover\": \"end\",\n",
        "        # \"number mover 4\": \"S4\",\n",
        "        \"number mover 3\": \"S3\",\n",
        "        \"number mover 2\": \"S2\",\n",
        "        \"number mover 1\": \"S1\",\n",
        "    }\n",
        "\n",
        "    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n",
        "\n",
        "    # ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n",
        "\n",
        "    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n",
        "    ioi_logits_minimal = model(dataset.toks)\n",
        "\n",
        "    # orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n",
        "    new_score = logits_to_ave_logit_diff(ioi_logits_minimal, dataset)\n",
        "    if print_output:\n",
        "        # print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n",
        "        # print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n",
        "        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n",
        "    # return new_score\n",
        "    return 100 * new_score / orig_score"
      ],
      "metadata": {
        "id": "QLK5m1Ps70vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_circuit_forw(curr_circuit=None, orig_score=100, threshold=10):\n",
        "    # threshold is T, a %. if performance is less than T%, allow its removal\n",
        "    if curr_circuit == []:\n",
        "        # Start with full circuit\n",
        "        curr_circuit = [(layer, head) for layer in range(12) for head in range(12)]\n",
        "\n",
        "    for layer in range(0, 12):\n",
        "        for head in range(12):\n",
        "            if (layer, head) not in curr_circuit:\n",
        "                continue\n",
        "\n",
        "            # Copying the curr_circuit so we can iterate over one and modify the other\n",
        "            copy_circuit = curr_circuit.copy()\n",
        "\n",
        "            # Temporarily removing the current tuple from the copied circuit\n",
        "            copy_circuit.remove((layer, head))\n",
        "\n",
        "            new_score = mean_ablate_by_lst(copy_circuit, model, orig_score, print_output=False).item()\n",
        "\n",
        "            # print((layer,head), new_score)\n",
        "            # If the result is less than the threshold, remove the tuple from the original list\n",
        "            if (100 - new_score) < threshold:\n",
        "                curr_circuit.remove((layer, head))\n",
        "\n",
        "                print(\"\\nRemoved:\", (layer, head))\n",
        "                print(new_score)\n",
        "\n",
        "    return curr_circuit, new_score"
      ],
      "metadata": {
        "id": "ybrqaAul70vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_circuit_backw(curr_circuit=None, orig_score=100, threshold=10):\n",
        "    # threshold is T, a %. if performance is less than T%, allow its removal\n",
        "    if curr_circuit == []:\n",
        "        # Start with full circuit\n",
        "        curr_circuit = [(layer, head) for layer in range(12) for head in range(12)]\n",
        "\n",
        "    for layer in range(11, -1, -1):  # go thru all heads in a layer first\n",
        "        for head in range(12):\n",
        "            if (layer, head) not in curr_circuit:\n",
        "                continue\n",
        "\n",
        "            # Copying the curr_circuit so we can iterate over one and modify the other\n",
        "            copy_circuit = curr_circuit.copy()\n",
        "\n",
        "            # Temporarily removing the current tuple from the copied circuit\n",
        "            copy_circuit.remove((layer, head))\n",
        "\n",
        "            new_score = mean_ablate_by_lst(copy_circuit, model, orig_score, print_output=False).item()\n",
        "\n",
        "            # If the result is less than the threshold, remove the tuple from the original list\n",
        "            if (100 - new_score) < threshold:\n",
        "                curr_circuit.remove((layer, head))\n",
        "\n",
        "                print(\"\\nRemoved:\", (layer, head))\n",
        "                print(new_score)\n",
        "\n",
        "    return curr_circuit, new_score"
      ],
      "metadata": {
        "id": "p7jLJcMH70vi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}