{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","collapsed_sections":["b3Chees1fkO1","VDfzJNjP66tK"],"machine_shape":"hm","authorship_tag":"ABX9TyM1YoIYT3c7ohBVxeSDpUAM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# run this to add files without wiating for setup. after adding, run all\n","1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e2itLOY5xX24","executionInfo":{"status":"ok","timestamp":1702514531757,"user_tz":300,"elapsed":383,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"3d6149f2-3320-4053-dd04-0461a285c9c8"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"DcZG9rm2IAiA"},"source":["# Setup\n","(No need to change anything)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rMcpSDdjIAiA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ac823b8f-039f-4c5a-9012-917d843fe11d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running as a Colab notebook\n","Collecting git+https://github.com/neelnanda-io/TransformerLens.git\n","  Cloning https://github.com/neelnanda-io/TransformerLens.git to /tmp/pip-req-build-ibp9p8tj\n","  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/TransformerLens.git /tmp/pip-req-build-ibp9p8tj\n","  Resolved https://github.com/neelnanda-io/TransformerLens.git to commit ce82675a8e89b6d5e6229a89620c843c794f3b04\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting accelerate>=0.23.0 (from transformer-lens==0.0.0)\n","  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting beartype<0.15.0,>=0.14.1 (from transformer-lens==0.0.0)\n","  Downloading beartype-0.14.1-py3-none-any.whl (739 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets>=2.7.1 (from transformer-lens==0.0.0)\n","  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting einops>=0.6.0 (from transformer-lens==0.0.0)\n","  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fancy-einsum>=0.0.3 (from transformer-lens==0.0.0)\n","  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n","Collecting jaxtyping>=0.2.11 (from transformer-lens==0.0.0)\n","  Downloading jaxtyping-0.2.24-py3-none-any.whl (38 kB)\n","Collecting numpy>=1.24 (from transformer-lens==0.0.0)\n","  Downloading numpy-1.26.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (1.5.3)\n","Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (13.7.0)\n","Collecting torch!=2.0,!=2.1.0,>=1.10 (from transformer-lens==0.0.0)\n","  Downloading torch-2.1.1-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (4.66.1)\n","Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (4.35.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (4.5.0)\n","Collecting wandb>=0.13.5 (from transformer-lens==0.0.0)\n","  Downloading wandb-0.16.1-py3-none-any.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (6.0.1)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (0.19.4)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (0.4.1)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (10.0.1)\n","Collecting pyarrow-hotfix (from datasets>=2.7.1->transformer-lens==0.0.0)\n","  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets>=2.7.1->transformer-lens==0.0.0)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (2.31.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (3.4.1)\n","Collecting multiprocess (from datasets>=2.7.1->transformer-lens==0.0.0)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (3.9.1)\n","Collecting typeguard<3,>=2.13.3 (from jaxtyping>=0.2.11->transformer-lens==0.0.0)\n","  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2023.3.post1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer-lens==0.0.0) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer-lens==0.0.0) (2.16.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (3.13.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (3.1.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["# Janky code to do different setup when run in a Colab notebook vs VSCode\n","DEBUG_MODE = False\n","try:\n","    import google.colab\n","    IN_COLAB = True\n","    print(\"Running as a Colab notebook\")\n","    %pip install git+https://github.com/neelnanda-io/TransformerLens.git\n","    # Install another version of node that makes PySvelte work way faster\n","    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n","    # %pip install git+https://github.com/neelnanda-io/PySvelte.git\n","except:\n","    IN_COLAB = False\n","    print(\"Running as a Jupyter notebook - intended for development only!\")\n","    from IPython import get_ipython\n","\n","    ipython = get_ipython()\n","    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n","    ipython.magic(\"load_ext autoreload\")\n","    ipython.magic(\"autoreload 2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xKoTs7VBIAiD"},"outputs":[],"source":["# # Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n","# import plotly.io as pio\n","\n","# if IN_COLAB or not DEBUG_MODE:\n","#     # Thanks to annoying rendering issues, Plotly graphics will either show up in colab OR Vscode depending on the renderer - this is bad for developing demos! Thus creating a debug mode.\n","#     pio.renderers.default = \"colab\"\n","# else:\n","#     pio.renderers.default = \"png\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z6b1n2tvIAiD"},"outputs":[],"source":["# Import stuff\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import einops\n","from fancy_einsum import einsum\n","import tqdm.notebook as tqdm\n","import random\n","from pathlib import Path\n","import plotly.express as px\n","from torch.utils.data import DataLoader\n","\n","from jaxtyping import Float, Int\n","from typing import List, Union, Optional\n","from functools import partial\n","import copy\n","\n","import itertools\n","from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n","import dataclasses\n","import datasets\n","from IPython.display import HTML"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zuhzYxbsIAiE"},"outputs":[],"source":["# import pysvelte\n","\n","import transformer_lens\n","import transformer_lens.utils as utils\n","from transformer_lens.hook_points import (\n","    HookedRootModule,\n","    HookPoint,\n",")  # Hooking utilities\n","from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"]},{"cell_type":"markdown","metadata":{"id":"hccba0v-IAiF"},"source":["We turn automatic differentiation off, to save GPU memory, as this notebook focuses on model inference not model training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cFMTUcQiIAiF"},"outputs":[],"source":["torch.set_grad_enabled(False)"]},{"cell_type":"markdown","metadata":{"id":"zyKb4C51IAiG"},"source":["Plotting helper functions:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KFs9BrbzIAiH"},"outputs":[],"source":["# def imshow(tensor, renderer=None, **kwargs):\n","#     px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", **kwargs).show(renderer)\n","\n","# def line(tensor, renderer=None, **kwargs):\n","#     px.line(y=utils.to_numpy(tensor), **kwargs).show(renderer)\n","\n","# def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n","#     x = utils.to_numpy(x)\n","#     y = utils.to_numpy(y)\n","#     px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"]},{"cell_type":"markdown","source":["## Load Model"],"metadata":{"id":"OLkInsdjyHMx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xLwDyosvIAiJ"},"outputs":[],"source":["model = HookedTransformer.from_pretrained(\n","    \"gpt2-small\",\n","    center_unembed=True,\n","    center_writing_weights=True,\n","    fold_ln=True,\n","    refactor_factored_attn_matrices=True,\n",")"]},{"cell_type":"markdown","source":["## Import functions from repo"],"metadata":{"id":"Z4iJEGh6b56v"}},{"cell_type":"code","source":["!git clone https://github.com/callummcdougall/ARENA_2.0.git"],"metadata":{"id":"Fdh5--MfYw7-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cd ARENA_2.0/chapter1_transformers/exercises/part3_indirect_object_identification"],"metadata":{"id":"iZ4C_bsXZFfj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import ioi_circuit_extraction as ioi_circuit_extraction"],"metadata":{"id":"OT0Sn571ZnkV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Generate dataset with multiple prompts"],"metadata":{"id":"6Fuq8XW770vX"}},{"cell_type":"code","source":["class Dataset:\n","    def __init__(self, prompts, pos_dict, tokenizer, S1_is_first=False):\n","        self.prompts = prompts\n","        self.tokenizer = tokenizer\n","        self.N = len(prompts)\n","        self.max_len = max(\n","            [\n","                len(self.tokenizer(prompt[\"text\"]).input_ids)\n","                for prompt in self.prompts\n","            ]\n","        )\n","        # all_ids = [prompt[\"TEMPLATE_IDX\"] for prompt in self.ioi_prompts]\n","        all_ids = [0 for prompt in self.prompts] # only 1 template\n","        all_ids_ar = np.array(all_ids)\n","        self.groups = []\n","        for id in list(set(all_ids)):\n","            self.groups.append(np.where(all_ids_ar == id)[0])\n","\n","        texts = [ prompt[\"text\"] for prompt in self.prompts ]\n","        self.toks = torch.Tensor(self.tokenizer(texts, padding=True).input_ids).type(\n","            torch.int\n","        )\n","        self.corr_tokenIDs = [\n","            # self.tokenizer.encode(\" \" + prompt[\"corr\"])[0] for prompt in self.prompts\n","            self.tokenizer.encode(prompt[\"corr\"])[0] for prompt in self.prompts\n","        ]\n","        self.incorr_tokenIDs = [\n","            # self.tokenizer.encode(\" \" + prompt[\"incorr\"])[0] for prompt in self.prompts\n","            self.tokenizer.encode(prompt[\"incorr\"])[0] for prompt in self.prompts\n","        ]\n","\n","        # word_idx: for every prompt, find the token index of each target token and \"end\"\n","        # word_idx is a tensor with an element for each prompt. The element is the targ token's ind at that prompt\n","        self.word_idx = {}\n","        # for targ in [key for key in self.prompts[0].keys() if (key != 'text' and key != 'corr' and key != 'incorr')]:\n","        for targ in [key for key in pos_dict]:\n","            targ_lst = []\n","            for prompt in self.prompts:\n","                input_text = prompt[\"text\"]\n","                tokens = model.tokenizer.tokenize(input_text)\n","                # if S1_is_first and targ == \"S1\":  # only use this if first token doesn't have space Ġ in front\n","                #     target_token = prompt[targ]\n","                # else:\n","                #     target_token = \"Ġ\" + prompt[targ]\n","                # target_index = tokens.index(target_token)\n","                target_index = pos_dict[targ]\n","                targ_lst.append(target_index)\n","            self.word_idx[targ] = torch.tensor(targ_lst)\n","\n","        targ_lst = []\n","        for prompt in self.prompts:\n","            input_text = prompt[\"text\"]\n","            tokens = self.tokenizer.tokenize(input_text)\n","            end_token_index = len(tokens) - 1\n","            targ_lst.append(end_token_index)\n","        self.word_idx[\"end\"] = torch.tensor(targ_lst)\n","\n","    def __len__(self):\n","        return self.N"],"metadata":{"id":"4wXBNWj5FwVn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","\n","prompts_list = []\n","\n","temps = ['done', 'lost', 'names']\n","\n","for i in temps:\n","    file_name = f'/content/digits_prompts_{i}.pkl'\n","    with open(file_name, 'rb') as file:\n","        filelist = pickle.load(file)\n","\n","    print(filelist[0]['text'])\n","    prompts_list += filelist [:512] #768 512\n","\n","len(prompts_list)"],"metadata":{"id":"CIe5yXuDhgEK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pos_dict = {\n","#     'S1': 4,\n","#     'S2': 10,\n","#     'S3': 16,\n","#     'S4': 22,\n","# }\n","\n","pos_dict = {}\n","for i in range(len(model.tokenizer.tokenize(prompts_list[0]['text']))):\n","    pos_dict['S'+str(i)] = i\n","\n","# pos_dict"],"metadata":{"id":"kS_Tlrb_70vg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = Dataset(prompts_list, pos_dict, model.tokenizer, S1_is_first=True)"],"metadata":{"id":"u0NPSKcZ1iDe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","\n","def generate_prompts_list_corr(prompt_list):\n","    outlist = []\n","    for prompt_dict in prompts_list:\n","        r1 = random.randint(1, 12)\n","        r2 = random.randint(1, 12)\n","        while True:\n","            r3 = random.randint(1, 12)\n","            r4 = random.randint(1, 12)\n","            if r4 - 1 != r3:\n","                break\n","        new_text = prompt_dict['text'].replace(prompt_dict['S1'], str(r1)).replace(prompt_dict['S2'], str(r2)).replace(prompt_dict['S3'], str(r3)).replace(prompt_dict['S4'], str(r4))\n","        new_prompt_dict = {\n","            'S1': str(r1),\n","            'S2': str(r2),\n","            'S3': str(r3),\n","            'S4': str(r4),\n","            'corr': prompt_dict['corr'],\n","            'incorr': prompt_dict['incorr'],\n","            'text': new_text\n","        }\n","        outlist.append(new_prompt_dict)\n","    return outlist\n","prompts_list_2 = generate_prompts_list_corr(prompts_list)"],"metadata":{"id":"YqAdblFt1_tx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompts_list_2[0]"],"metadata":{"id":"Ry2G9aV9EnEn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(prompts_list_2)"],"metadata":{"id":"l6sRiXpRY9hL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","from google.colab import files\n","\n","with open('randDS_numerals.pkl', 'wb') as file:\n","    pickle.dump(prompts_list_2, file)\n","files.download('randDS_numerals.pkl')"],"metadata":{"id":"SeiO3sgDxC9q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer, S1_is_first=True)"],"metadata":{"id":"msu6D4p_feW5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# get orig score"],"metadata":{"id":"BHHvz84w70vh"}},{"cell_type":"code","source":["from torch import Tensor\n","\n","def logits_to_ave_logit_diff_2(logits: Float[Tensor, \"batch seq d_vocab\"], dataset: Dataset, per_prompt=False):\n","    '''\n","    Returns logit difference between the correct and incorrect answer.\n","\n","    If per_prompt=True, return the array of differences rather than the average.\n","    '''\n","\n","    # Only the final logits are relevant for the answer\n","    # Get the logits corresponding to the indirect object / subject tokens respectively\n","    corr_logits: Float[Tensor, \"batch\"] = logits[range(logits.size(0)), dataset.word_idx[\"end\"], dataset.corr_tokenIDs]\n","    incorr_logits: Float[Tensor, \"batch\"] = logits[range(logits.size(0)), dataset.word_idx[\"end\"], dataset.incorr_tokenIDs]\n","    # Find logit difference\n","    answer_logit_diff = corr_logits - incorr_logits\n","    return answer_logit_diff if per_prompt else answer_logit_diff.mean()"],"metadata":{"id":"AFYffMoP70vh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.reset_hooks(including_permanent=True)\n","# ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","ioi_logits_original = model(dataset.toks)\n","orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)"],"metadata":{"id":"OI3FcmpMaNxB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gc\n","\n","# del(ioi_cache)\n","del(ioi_logits_original)\n","\n","torch.cuda.empty_cache()\n","gc.collect()"],"metadata":{"id":"A-TjmW5PUwGC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["orig_score"],"metadata":{"id":"XfjQPngxBYBM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Ablation Expm Functions"],"metadata":{"id":"b3Chees1fkO1"}},{"cell_type":"code","source":["def mean_ablate_by_lst(lst, model, orig_score, print_output=True):\n","    # CIRCUIT = {\n","    #     \"number mover\": lst,\n","    #     \"number mover 4\": lst,\n","    #     \"number mover 3\": lst,\n","    #     \"number mover 2\": lst,\n","    #     \"number mover 1\": lst,\n","    # }\n","\n","    # SEQ_POS_TO_KEEP = {\n","    #     \"number mover\": \"end\",\n","    #     \"number mover 4\": \"S4\",\n","    #     \"number mover 3\": \"S3\",\n","    #     \"number mover 2\": \"S2\",\n","    #     \"number mover 1\": \"S1\",\n","    # }\n","    CIRCUIT = {}\n","    SEQ_POS_TO_KEEP = {}\n","    for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","        CIRCUIT['S'+str(i)] = lst\n","        if i == len(model.tokenizer.tokenize(prompts_list_2[0]['text'])) - 1:\n","            SEQ_POS_TO_KEEP['S'+str(i)] = 'end'\n","        else:\n","            SEQ_POS_TO_KEEP['S'+str(i)] = 'S'+str(i)\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    # ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    ioi_logits_minimal = model(dataset.toks)\n","\n","    # orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff_2(ioi_logits_minimal, dataset)\n","    # del(ioi_logits_minimal)\n","    if print_output:\n","        # print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        # print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    # return new_score\n","    return 100 * new_score / orig_score"],"metadata":{"id":"QLK5m1Ps70vh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# rmv most impt heads from full"],"metadata":{"id":"w82u8B4EZdWi"}},{"cell_type":"code","source":["circ = [(layer, head) for layer in range(12) for head in range(12)]\n","to_loop = [(7, 11), (4, 4), (1, 5), (10, 7), (8, 8), (8, 6), (9, 1), (6, 6), (8, 1), (6, 10)]\n","\n","lh_scores = {}\n","# for lh in circ:\n","for lh in to_loop:\n","    copy_circuit = circ.copy()\n","    copy_circuit.remove(lh)\n","    print(\"removed: \" + str(lh))\n","    new_score = mean_ablate_by_lst(copy_circuit, model, orig_score, print_output=True).item()\n","    lh_scores[lh] = new_score"],"metadata":{"id":"msckx6kcZgAd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sort the dictionary by values in descending order\n","sorted_lh_scores = dict(sorted(lh_scores.items(), key=lambda item: -item[1], reverse=True))\n","\n","# Select the top 10 items\n","top_10_lh_scores = dict(list(sorted_lh_scores.items())[:10])\n","top_10_lh_scores"],"metadata":{"id":"LTNyxnWMZgAd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for lh, score in sorted_lh_scores.items():\n","#     print(lh, -round(100-score, 2))\n","\n","# Sort the dictionary by values in descending order\n","sorted_lh_scores = sorted(lh_scores.items(), key=lambda item: -item[1], reverse=True)\n","\n","# Iterate over the top 10 items and print them\n","for lh, score in sorted_lh_scores[:10]:\n","    modified_score = -round(100 - score, 2)\n","    print(lh, modified_score)"],"metadata":{"id":"D3E9dzLOZgAe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","import matplotlib.pyplot as plt\n","\n","lh_scores_drop = {key: min(0, val-100) for key, val in lh_scores.items()}\n","\n","# Extracting only the values (scores) from the dictionary\n","scores = list(lh_scores_drop.values())\n","\n","# Creating a histogram for the scores\n","plt.hist(scores, bins=10, edgecolor='black')  # Adjust the number of bins as needed\n","\n","# Creating a histogram for the scores\n","n, bins, patches = plt.hist(scores, bins=10, edgecolor='black')  # Adjust the number of bins as needed\n","\n","# Annotating the histogram with the number of values in each bin\n","for i in range(len(n)):\n","    plt.text(bins[i]+5, n[i], str(int(n[i])), va='bottom', ha='center')\n","\n","# Setting x-axis ticks at intervals of 10 from 0 to 100\n","plt.xticks(range(-100, 0, 10))\n","\n","# Adding labels and title for clarity\n","plt.xlabel('Percentage Drop from Full Performance')\n","plt.ylabel('Number of Attention Heads')\n","# plt.title('Distribution of Attention Head Performance Drop Percentages')\n","\n","# Displaying the plot\n","# plt.show()\n","\n","# Save the figure\n","pdf_filename = 'lh_scores_distribution.pdf'\n","plt.savefig(pdf_filename)\n","\n","# Download the file in Colab\n","files.download(pdf_filename)"],"metadata":{"id":"yVAAmsZikKQV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import statistics\n","\n","# Assuming lh_scores_drop is already defined\n","# For example, lh_scores_drop = {key: max(0, 100 - val) for key, val in lh_scores.items()}\n","\n","# Extracting the values from the dictionary\n","scores = list(lh_scores_drop.values())\n","\n","# Calculating the mean\n","mean_score = statistics.mean(scores)\n","\n","print(\"Mean of the scores:\", mean_score)\n"],"metadata":{"id":"A6E_Lbomm1js"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def get_probability_ranking(value, distribution):\n","#     # Convert distribution to a probability distribution (if not already)\n","#     total = sum(distribution)\n","#     prob_distribution = [x / total for x in distribution]\n","\n","#     # Sort the probability distribution\n","#     sorted_distribution = sorted(prob_distribution)\n","\n","#     # Calculate the cumulative probability up to the given value\n","#     cumulative_prob = sum(prob for prob in sorted_distribution if prob <= value)\n","\n","#     return cumulative_prob\n","\n","# # Example usage\n","# distribution = lh_scores_drop.values()\n","# value = -0.69  # Value to get the probability ranking for\n","# ranking = get_probability_ranking(value, distribution)\n","# print(f\"Probability Ranking of {value} in the distribution: {ranking}\")\n"],"metadata":{"id":"74LSPw6fg4ru"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","\n","# Saving the dictionary to a file using pickle\n","with open('numerals_lh_scores.pkl', 'wb') as file:\n","    pickle.dump(lh_scores, file)\n","\n","from google.colab import files\n","\n","# Download the file to your local machine\n","files.download('numerals_lh_scores.pkl')"],"metadata":{"id":"HhKz7gEBcEqa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# MLP ablation fns"],"metadata":{"id":"G9FQY3H3zkFV"}},{"cell_type":"code","source":["from torch import Tensor\n","from typing import Dict, Tuple, List\n","from jaxtyping import Float, Bool\n","import torch as t\n","\n","def logits_to_ave_logit_diff(logits: Float[Tensor, \"batch seq d_vocab\"], dataset: Dataset, per_prompt=False):\n","    '''\n","    Returns logit difference between the correct and incorrect answer.\n","\n","    If per_prompt=True, return the array of differences rather than the average.\n","    '''\n","\n","    # Only the final logits are relevant for the answer\n","    # Get the logits corresponding to the indirect object / subject tokens respectively\n","    corr_logits: Float[Tensor, \"batch\"] = logits[range(logits.size(0)), dataset.word_idx[\"end\"], dataset.corr_tokenIDs]\n","    incorr_logits: Float[Tensor, \"batch\"] = logits[range(logits.size(0)), dataset.word_idx[\"end\"], dataset.incorr_tokenIDs]\n","    # Find logit difference\n","    answer_logit_diff = corr_logits - incorr_logits\n","    return answer_logit_diff if per_prompt else answer_logit_diff.mean()"],"metadata":{"id":"ZOTRN8KnheFO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_means_by_template_MLP(\n","    means_dataset: Dataset,\n","    model: HookedTransformer\n",") -> Float[Tensor, \"layer batch seq head_idx d_head\"]:\n","    '''\n","    Returns the mean of each head's output over the means dataset. This mean is\n","    computed separately for each group of prompts with the same template (these\n","    are given by means_dataset.groups).\n","    '''\n","    # Cache the outputs of every head\n","    _, means_cache = model.run_with_cache(\n","        means_dataset.toks.long(),\n","        return_type=None,\n","        names_filter=lambda name: name.endswith(\"mlp_out\"),\n","    )\n","    # Create tensor to store means\n","    n_layers, d_model = model.cfg.n_layers, model.cfg.d_model\n","    batch, seq_len = len(means_dataset), means_dataset.max_len\n","    means = t.zeros(size=(n_layers, batch, seq_len, d_model), device=model.cfg.device)\n","\n","    # Get set of different templates for this data\n","    for layer in range(n_layers):\n","        mlp_output_for_this_layer: Float[Tensor, \"batch seq d_model\"] = means_cache[utils.get_act_name(\"mlp_out\", layer)]\n","        for template_group in means_dataset.groups:  # here, we only have one group\n","            mlp_output_for_this_template = mlp_output_for_this_layer[template_group]\n","            # aggregate all batches\n","            mlp_output_means_for_this_template = einops.reduce(mlp_output_for_this_template, \"batch seq d_model -> seq d_model\", \"mean\")\n","            means[layer, template_group] = mlp_output_means_for_this_template\n","            # at layer, each batch ind is tempalte group (a tensor of size seq d_model)\n","            # is assigned the SAME mean, \"mlp_output_means_for_this_template\"\n","\n","    del(means_cache)\n","\n","    return means"],"metadata":{"id":"McmRZoY7Wudl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_mlp_outputs_and_posns_to_keep(\n","    means_dataset: Dataset,\n","    model: HookedTransformer,\n","    circuit: Dict[str, List[int]],  # Adjusted to hold list of layers instead of (layer, head) tuples\n","    seq_pos_to_keep: Dict[str, str],\n",") -> Dict[int, Bool[Tensor, \"batch seq\"]]:  # Adjusted the return type to \"batch seq\"\n","    '''\n","    Returns a dictionary mapping layers to a boolean mask giving the indices of the\n","    MLP output which *shouldn't* be mean-ablated.\n","\n","    The output of this function will be used for the hook function that does ablation.\n","    '''\n","    mlp_outputs_and_posns_to_keep = {}\n","    batch, seq = len(means_dataset), means_dataset.max_len\n","\n","    for layer in range(model.cfg.n_layers):\n","        mask = t.zeros(size=(batch, seq))\n","\n","        for (mlp_type, layer_list) in circuit.items():\n","            seq_pos = seq_pos_to_keep[mlp_type]\n","            indices = means_dataset.word_idx[seq_pos]\n","            if layer in layer_list:  # Check if the current layer is in the layer list for this mlp_type\n","                mask[:, indices] = 1\n","\n","        mlp_outputs_and_posns_to_keep[layer] = mask.bool()\n","\n","    return mlp_outputs_and_posns_to_keep"],"metadata":{"id":"MH4KI_wCu7M-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def hook_fn_mask_mlp_out(\n","    mlp_out: Float[Tensor, \"batch seq d_mlp\"],\n","    hook: HookPoint,\n","    mlp_outputs_and_posns_to_keep: Dict[int, Bool[Tensor, \"batch seq\"]],\n","    means: Float[Tensor, \"layer batch seq d_mlp\"],\n",") -> Float[Tensor, \"batch seq d_mlp\"]:\n","    '''\n","    Hook function which masks the MLP output of a transformer layer.\n","\n","    mlp_outputs_and_posns_to_keep\n","        Dict created with the get_mlp_outputs_and_posns_to_keep function. This tells\n","        us where to mask.\n","\n","    means\n","        Tensor of mean MLP output values of the means_dataset over each group of prompts\n","        with the same template. This tells us what values to mask with.\n","    '''\n","    # Get the mask for this layer, adapted for MLP output structure\n","    mask_for_this_layer = mlp_outputs_and_posns_to_keep[hook.layer()].unsqueeze(-1).to(mlp_out.device)\n","\n","    # Set MLP output values to the mean where necessary\n","    mlp_out = t.where(mask_for_this_layer, mlp_out, means[hook.layer()])\n","\n","    return mlp_out"],"metadata":{"id":"fXWq7V0Mv0F4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CIRCUIT = {}\n","SEQ_POS_TO_KEEP = {}\n","def add_mean_ablation_hook_MLP(\n","    model: HookedTransformer,\n","    means_dataset: Dataset,\n","    circuit: Dict[str, List[Tuple[int, int]]] = CIRCUIT,\n","    seq_pos_to_keep: Dict[str, str] = SEQ_POS_TO_KEEP,\n","    is_permanent: bool = True,\n",") -> HookedTransformer:\n","    '''\n","    Adds a permanent hook to the model, which ablates according to the circuit and\n","    seq_pos_to_keep dictionaries.\n","\n","    In other words, when the model is run on ioi_dataset, every head's output will\n","    be replaced with the mean over means_dataset for sequences with the same template,\n","    except for a subset of heads and sequence positions as specified by the circuit\n","    and seq_pos_to_keep dicts.\n","    '''\n","\n","    model.reset_hooks(including_permanent=True)\n","\n","    # Compute the mean of each head's output on the ABC dataset, grouped by template\n","    means = compute_means_by_template_MLP(means_dataset, model)\n","\n","    # Convert this into a boolean map\n","    mlp_outputs_and_posns_to_keep = get_mlp_outputs_and_posns_to_keep(means_dataset, model, circuit, seq_pos_to_keep)\n","\n","    # Get a hook function which will patch in the mean z values for each head, at\n","    # all positions which aren't important for the circuit\n","    hook_fn = partial(\n","        hook_fn_mask_mlp_out,\n","        mlp_outputs_and_posns_to_keep=mlp_outputs_and_posns_to_keep,\n","        means=means\n","    )\n","\n","    # Apply hook\n","    model.add_hook(lambda name: name.endswith(\"mlp_out\"), hook_fn, is_permanent=True)\n","\n","    return model"],"metadata":{"id":"sJlawX18v-yD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def mean_ablate_by_lst_MLP(lst, model, orig_score, print_output=True):\n","    CIRCUIT = {}\n","    SEQ_POS_TO_KEEP = {}\n","    for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","        CIRCUIT['S'+str(i)] = lst\n","        if i == len(model.tokenizer.tokenize(prompts_list_2[0]['text'])) - 1:\n","            SEQ_POS_TO_KEEP['S'+str(i)] = 'end'\n","        else:\n","            SEQ_POS_TO_KEEP['S'+str(i)] = 'S'+str(i)\n","\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","    # ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n","\n","    model = add_mean_ablation_hook_MLP(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n","    new_logits = model(dataset.toks)\n","\n","    # orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n","    new_score = logits_to_ave_logit_diff(new_logits, dataset)\n","    del(new_logits)\n","    if print_output:\n","        # print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n","        # print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n","        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n","    # return new_score\n","    return 100 * new_score / orig_score"],"metadata":{"id":"Ko6itvH15NtO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# loop thru MLPs from full"],"metadata":{"id":"zkx8xD8dwWOL"}},{"cell_type":"code","source":["for i in range(12):\n","    lst = [layer for layer in range(12) if layer != i]\n","    perc_of_orig = mean_ablate_by_lst_MLP(lst, model, orig_score, print_output=False).item()\n","    print(i, perc_of_orig)"],"metadata":{"id":"I9SR5ETh6BWw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# MLP and Head together fns"],"metadata":{"id":"DlpH0Wib-v1j"}},{"cell_type":"markdown","source":["## head fns"],"metadata":{"id":"Zv5yGHXhpAK_"}},{"cell_type":"code","source":["def get_heads_and_posns_to_keep(\n","    means_dataset: Dataset,\n","    model: HookedTransformer,\n","    circuit: Dict[str, List[Tuple[int, int]]],\n","    seq_pos_to_keep: Dict[str, str],\n",") -> Dict[int, Bool[Tensor, \"batch seq head\"]]:\n","    '''\n","    Returns a dictionary mapping layers to a boolean mask giving the indices of the\n","    z output which *shouldn't* be mean-ablated.\n","\n","    The output of this function will be used for the hook function that does ablation.\n","    '''\n","    heads_and_posns_to_keep = {}\n","    batch, seq, n_heads = len(means_dataset), means_dataset.max_len, model.cfg.n_heads\n","\n","    for layer in range(model.cfg.n_layers):\n","\n","        mask = t.zeros(size=(batch, seq, n_heads))\n","\n","        for (head_type, head_list) in circuit.items():\n","            seq_pos = seq_pos_to_keep[head_type]\n","            indices = means_dataset.word_idx[seq_pos] # modify this for key vs query pos. curr, this is query\n","            for (layer_idx, head_idx) in head_list:\n","                if layer_idx == layer:\n","                    mask[:, indices, head_idx] = 1\n","\n","        heads_and_posns_to_keep[layer] = mask.bool()\n","\n","    return heads_and_posns_to_keep\n","\n","def hook_fn_mask_z(\n","    z: Float[Tensor, \"batch seq head d_head\"],\n","    hook: HookPoint,\n","    heads_and_posns_to_keep: Dict[int, Bool[Tensor, \"batch seq head\"]],\n","    means: Float[Tensor, \"layer batch seq head d_head\"],\n",") -> Float[Tensor, \"batch seq head d_head\"]:\n","    '''\n","    Hook function which masks the z output of a transformer head.\n","\n","    heads_and_posns_to_keep\n","        Dict created with the get_heads_and_posns_to_keep function. This tells\n","        us where to mask.\n","\n","    means\n","        Tensor of mean z values of the means_dataset over each group of prompts\n","        with the same template. This tells us what values to mask with.\n","    '''\n","    # Get the mask for this layer, and add d_head=1 dimension so it broadcasts correctly\n","    mask_for_this_layer = heads_and_posns_to_keep[hook.layer()].unsqueeze(-1).to(z.device)\n","\n","    # Set z values to the mean\n","    z = t.where(mask_for_this_layer, z, means[hook.layer()])\n","\n","    return z\n","\n","def compute_means_by_template(\n","    means_dataset: Dataset,\n","    model: HookedTransformer\n",") -> Float[Tensor, \"layer batch seq head_idx d_head\"]:\n","    '''\n","    Returns the mean of each head's output over the means dataset. This mean is\n","    computed separately for each group of prompts with the same template (these\n","    are given by means_dataset.groups).\n","    '''\n","    # Cache the outputs of every head\n","    _, means_cache = model.run_with_cache(\n","        means_dataset.toks.long(),\n","        return_type=None,\n","        names_filter=lambda name: name.endswith(\"z\"),\n","    )\n","    # Create tensor to store means\n","    n_layers, n_heads, d_head = model.cfg.n_layers, model.cfg.n_heads, model.cfg.d_head\n","    batch, seq_len = len(means_dataset), means_dataset.max_len\n","    means = t.zeros(size=(n_layers, batch, seq_len, n_heads, d_head), device=model.cfg.device)\n","\n","    # Get set of different templates for this data\n","    for layer in range(model.cfg.n_layers):\n","        z_for_this_layer: Float[Tensor, \"batch seq head d_head\"] = means_cache[utils.get_act_name(\"z\", layer)]\n","        for template_group in means_dataset.groups:\n","            z_for_this_template = z_for_this_layer[template_group]\n","            z_means_for_this_template = einops.reduce(z_for_this_template, \"batch seq head d_head -> seq head d_head\", \"mean\")\n","            means[layer, template_group] = z_means_for_this_template\n","\n","    del(means_cache)\n","\n","    return means\n","\n","def add_mean_ablation_hook(\n","    model: HookedTransformer,\n","    means_dataset: Dataset,\n","    circuit: Dict[str, List[Tuple[int, int]]] = CIRCUIT,\n","    seq_pos_to_keep: Dict[str, str] = SEQ_POS_TO_KEEP,\n","    is_permanent: bool = True,\n",") -> HookedTransformer:\n","    '''\n","    Adds a permanent hook to the model, which ablates according to the circuit and\n","    seq_pos_to_keep dictionaries.\n","\n","    In other words, when the model is run on ioi_dataset, every head's output will\n","    be replaced with the mean over means_dataset for sequences with the same template,\n","    except for a subset of heads and sequence positions as specified by the circuit\n","    and seq_pos_to_keep dicts.\n","    '''\n","\n","    model.reset_hooks(including_permanent=True)\n","\n","    # Compute the mean of each head's output on the ABC dataset, grouped by template\n","    means = compute_means_by_template(means_dataset, model)\n","\n","    # Convert this into a boolean map\n","    heads_and_posns_to_keep = get_heads_and_posns_to_keep(means_dataset, model, circuit, seq_pos_to_keep)\n","\n","    # Get a hook function which will patch in the mean z values for each head, at\n","    # all positions which aren't important for the circuit\n","    hook_fn = partial(\n","        hook_fn_mask_z,\n","        heads_and_posns_to_keep=heads_and_posns_to_keep,\n","        means=means\n","    )\n","\n","    # Apply hook\n","    model.add_hook(lambda name: name.endswith(\"z\"), hook_fn, is_permanent=is_permanent)\n","\n","    return model"],"metadata":{"id":"cE7xLtws_Pnt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## both"],"metadata":{"id":"4zlcncHKo2h6"}},{"cell_type":"code","source":["def add_mean_ablation_hook_MLP_head(\n","    model: HookedTransformer,\n","    means_dataset: Dataset,\n","    heads_lst, mlp_lst,\n","    is_permanent: bool = True,\n",") -> HookedTransformer:\n","    CIRCUIT = {}\n","    SEQ_POS_TO_KEEP = {}\n","    for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","        CIRCUIT['S'+str(i)] = heads_lst\n","        if i == len(model.tokenizer.tokenize(prompts_list_2[0]['text'])) - 1:\n","            SEQ_POS_TO_KEEP['S'+str(i)] = 'end'\n","        else:\n","            SEQ_POS_TO_KEEP['S'+str(i)] = 'S'+str(i)\n","\n","    model.reset_hooks(including_permanent=True)\n","\n","    # Compute the mean of each head's output on the ABC dataset, grouped by template\n","    means = compute_means_by_template(means_dataset, model)\n","\n","    # Convert this into a boolean map\n","    heads_and_posns_to_keep = get_heads_and_posns_to_keep(means_dataset, model, CIRCUIT, SEQ_POS_TO_KEEP)\n","\n","    # Get a hook function which will patch in the mean z values for each head, at\n","    # all positions which aren't important for the circuit\n","    hook_fn = partial(\n","        hook_fn_mask_z,\n","        heads_and_posns_to_keep=heads_and_posns_to_keep,\n","        means=means\n","    )\n","\n","    # Apply hook\n","    model.add_hook(lambda name: name.endswith(\"z\"), hook_fn, is_permanent=is_permanent)\n","\n","    ########################\n","    CIRCUIT = {}\n","    SEQ_POS_TO_KEEP = {}\n","    for i in range(len(model.tokenizer.tokenize(prompts_list_2[0]['text']))):\n","        CIRCUIT['S'+str(i)] = mlp_lst\n","        if i == len(model.tokenizer.tokenize(prompts_list_2[0]['text'])) - 1:\n","            SEQ_POS_TO_KEEP['S'+str(i)] = 'end'\n","        else:\n","            SEQ_POS_TO_KEEP['S'+str(i)] = 'S'+str(i)\n","\n","    # Compute the mean of each head's output on the ABC dataset, grouped by template\n","    means = compute_means_by_template_MLP(means_dataset, model)\n","\n","    # Convert this into a boolean map\n","    mlp_outputs_and_posns_to_keep = get_mlp_outputs_and_posns_to_keep(means_dataset, model, CIRCUIT, SEQ_POS_TO_KEEP)\n","\n","    # Get a hook function which will patch in the mean z values for each head, at\n","    # all positions which aren't important for the circuit\n","    hook_fn = partial(\n","        hook_fn_mask_mlp_out,\n","        mlp_outputs_and_posns_to_keep=mlp_outputs_and_posns_to_keep,\n","        means=means\n","    )\n","\n","    # Apply hook\n","    model.add_hook(lambda name: name.endswith(\"mlp_out\"), hook_fn, is_permanent=True)\n","\n","    return model"],"metadata":{"id":"mnGaDWe__CyA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## loop iters"],"metadata":{"id":"049GQVcZoyrI"}},{"cell_type":"code","source":["def find_circuit_forw(heads_not_ablate=None, mlps_not_ablate=None, orig_score=100, threshold=10):\n","    # threshold is T, a %. if performance is less than T%, allow its removal\n","    # we don't ablate the curr circuits\n","    if heads_not_ablate == []: # Start with full circuit\n","        heads_not_ablate = [(layer, head) for layer in range(12) for head in range(12)]\n","    if mlps_not_ablate == []:\n","        mlps_not_ablate = [layer for layer in range(12)]\n","\n","    comp_scores = {}\n","    for layer in range(0, 12):\n","        for head in range(12):\n","            print(layer, head)\n","            if (layer, head) not in heads_not_ablate:\n","                continue\n","\n","            copy_heads_not_ablate = heads_not_ablate.copy()\n","            copy_heads_not_ablate.remove((layer, head))\n","\n","            model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","            ablated_model = add_mean_ablation_hook_MLP_head(model, dataset_2, copy_heads_not_ablate, mlps_not_ablate)\n","\n","            new_logits = ablated_model(dataset.toks)\n","            new_score = logits_to_ave_logit_diff(new_logits, dataset)\n","            new_perc = 100 * new_score / orig_score\n","            comp_scores[layer] = new_perc\n","            print(f\"(cand circuit / full) %: {new_perc:.4f}\")\n","            if (100 - new_perc) < threshold:\n","                heads_not_ablate.remove((layer, head))\n","                print(\"Removed:\", (layer, head))\n","            del(new_logits)\n","\n","        print(layer)\n","        if layer in mlps_not_ablate:\n","            copy_mlps_not_ablate = mlps_not_ablate.copy()\n","            copy_mlps_not_ablate.remove(layer)\n","\n","            model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","            ablated_model = add_mean_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, copy_mlps_not_ablate)\n","\n","            new_logits = ablated_model(dataset.toks)\n","            new_score = logits_to_ave_logit_diff(new_logits, dataset)\n","            new_perc = 100 * new_score / orig_score\n","            comp_scores[(layer, head)] = new_perc\n","            print(f\"(cand circuit / full) %: {new_perc:.4f}\")\n","            if (100 - new_perc) < threshold:\n","                mlps_not_ablate.remove(layer)\n","                print(\"Removed: MLP \", layer)\n","            del(new_logits)\n","\n","    return heads_not_ablate, mlps_not_ablate, new_perc, comp_scores"],"metadata":{"id":"Tx5xoj2HtT4C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def find_circuit_backw(heads_not_ablate=None, mlps_not_ablate=None, orig_score=100, threshold=10):\n","    # threshold is T, a %. if performance is less than T%, allow its removal\n","    # we don't ablate the curr circuits\n","    if heads_not_ablate == []: # Start with full circuit\n","        heads_not_ablate = [(layer, head) for layer in range(12) for head in range(12)]\n","    if mlps_not_ablate == []:\n","        mlps_not_ablate = [layer for layer in range(12)]\n","\n","    comp_scores = {}\n","    for layer in range(11, -1, -1):  # go thru all heads in a layer first\n","        print(layer)\n","        if layer in mlps_not_ablate:\n","            copy_mlps_not_ablate = mlps_not_ablate.copy()\n","            copy_mlps_not_ablate.remove(layer)\n","\n","            model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","            ablated_model = add_mean_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, copy_mlps_not_ablate)\n","\n","            new_logits = ablated_model(dataset.toks)\n","            new_score = logits_to_ave_logit_diff(new_logits, dataset)\n","            new_perc = 100 * new_score / orig_score\n","            comp_scores[layer] = new_perc\n","            print(f\"(cand circuit / full) %: {new_perc:.4f}\")\n","            if (100 - new_perc) < threshold:\n","                mlps_not_ablate.remove(layer)\n","                print(\"Removed: MLP \", layer)\n","            del(new_logits)\n","\n","        for head in range(12):\n","            print(layer, head)\n","            if (layer, head) not in heads_not_ablate:\n","                continue\n","\n","            copy_heads_not_ablate = heads_not_ablate.copy()\n","            copy_heads_not_ablate.remove((layer, head))\n","\n","            model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","            ablated_model = add_mean_ablation_hook_MLP_head(model, dataset_2, copy_heads_not_ablate, mlps_not_ablate)\n","\n","            new_logits = ablated_model(dataset.toks)\n","            new_score = logits_to_ave_logit_diff(new_logits, dataset)\n","            new_perc = 100 * new_score / orig_score\n","            comp_scores[(layer, head)] = new_perc\n","            print(f\"(cand circuit / full) %: {new_perc:.4f}\")\n","            if (100 - new_perc) < threshold:\n","                heads_not_ablate.remove((layer, head))\n","                print(\"Removed:\", (layer, head))\n","            del(new_logits)\n","\n","    return heads_not_ablate, mlps_not_ablate, new_score, comp_scores"],"metadata":{"id":"hJYEoSI3ox5l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# run MLP and Head together"],"metadata":{"id":"VDfzJNjP66tK"}},{"cell_type":"code","source":["heads_not_ablate = [(layer, head) for layer in range(12) for head in range(12)]\n","mlps_not_ablate = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n","\n","ablated_model = add_mean_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","new_logits = ablated_model(dataset.toks)\n","new_score = logits_to_ave_logit_diff(new_logits, dataset)\n","print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")"],"metadata":{"id":"pfjSc0rEALmb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["del(new_logits)"],"metadata":{"id":"kPHjfqyKpXGd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get rid of last layer\n","\n","heads_not_ablate = [(layer, head) for layer in range(11) for head in range(12)]\n","mlps_not_ablate = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n","\n","ablated_model = add_mean_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","new_logits = ablated_model(dataset.toks)\n","new_score = logits_to_ave_logit_diff(new_logits, dataset)\n","print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")"],"metadata":{"id":"RHbM_oeSAsbO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# iter backw fwd, threshold 20"],"metadata":{"id":"0NYZB-G19liQ"}},{"cell_type":"code","source":["threshold = 20\n","curr_circ_heads = []\n","curr_circ_mlps = []\n","prev_score = 100\n","new_score = 0\n","iter = 1\n","all_comp_scores = []\n","while prev_score != new_score:\n","    print('\\nbackw prune, iter ', str(iter))\n","    # prev_score = new_score # save old score before finding new one\n","    old_circ_heads = curr_circ_heads.copy() # save old before finding new one\n","    old_circ_mlps = curr_circ_mlps.copy()\n","    curr_circ_heads, curr_circ_mlps, new_score, comp_scores = find_circuit_backw(curr_circ_heads, curr_circ_mlps, orig_score, threshold)\n","    if old_circ_heads == curr_circ_heads and old_circ_mlps == curr_circ_mlps:\n","        break\n","    all_comp_scores.append(comp_scores)\n","    print('\\nfwd prune, iter ', str(iter))\n","    # track changes in circuit as for some reason it doesn't work with scores\n","    old_circ_heads = curr_circ_heads.copy()\n","    old_circ_mlps = curr_circ_mlps.copy()\n","    curr_circ_heads, curr_circ_mlps, new_score, comp_scores = find_circuit_forw(curr_circ_heads, curr_circ_mlps, orig_score, threshold)\n","    if old_circ_heads == curr_circ_heads and old_circ_mlps == curr_circ_mlps:\n","        break\n","    all_comp_scores.append(comp_scores)\n","    iter += 1"],"metadata":{"id":"gUqLZl-QsahY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","from google.colab import files\n","\n","with open('numerals_bf_20_scores.pkl', 'wb') as file:\n","    pickle.dump(all_comp_scores, file)\n","files.download('numerals_bf_20_scores.pkl')"],"metadata":{"id":"V93FIJs2MiU9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["curr_circ_heads"],"metadata":{"id":"oPlS7M_vyBSG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["curr_circ_mlps"],"metadata":{"id":"zrSFmCgtyFwY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## loop rmv and check for most impt heads"],"metadata":{"id":"8At2Kqx69liS"}},{"cell_type":"code","source":["model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","model = add_mean_ablation_hook_MLP_head(model, dataset_2, curr_circ_heads, curr_circ_mlps)\n","\n","new_logits = model(dataset.toks)\n","new_score = logits_to_ave_logit_diff(new_logits, dataset)\n","circ_score = (100 * new_score / orig_score).item()\n","print(f\"(cand circuit / full) %: {circ_score:.4f}\")\n","\n","# len(curr_circ_heads)\n","# len(curr_circ_mlps)"],"metadata":{"id":"ivoDzNKY9liS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lh_scores = {}\n","for lh in circ:\n","    copy_circuit = curr_circ_heads.copy()\n","    copy_circuit.remove(lh)\n","    print(\"removed: \" + str(lh))\n","    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","    model = add_mean_ablation_hook_MLP_head(model, dataset_2, copy_circuit, curr_circ_mlps)\n","\n","    new_logits = model(dataset.toks)\n","    new_score = logits_to_ave_logit_diff(new_logits, dataset).item()\n","    new_perc = 100 * new_score / orig_score\n","    print(f\"(cand circuit / full) %: {new_perc:.4f}\")\n","    lh_scores[lh] = new_perc"],"metadata":{"id":"vsUtHR-y9liS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sorted_lh_scores = dict(sorted(lh_scores.items(), key=lambda item: item[1]))\n","sorted_lh_scores"],"metadata":{"id":"MNzdWLFj9liT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for lh, score in sorted_lh_scores.items():\n","    print(lh, -round(circ_score-score, 2))"],"metadata":{"id":"RPCynBNH9liT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# try other tasks circs, thres 20"],"metadata":{"id":"xbZkzn0nrrxt"}},{"cell_type":"code","source":["# digits incr\n","# incorr i+3\n","circuit = [(0, 1), (0, 2), (0, 5), (0, 7), (0, 8), (0, 10), (1, 0), (1, 1), (1, 3), (1, 5), (1, 7), (1, 11), (2, 0), (2, 1), (2, 2), (2, 3), (2, 5), (2, 6), (2, 8), (2, 9), (2, 10), (3, 3), (3, 7), (3, 8), (3, 10), (3, 11), (4, 2), (4, 4), (4, 6), (4, 10), (4, 11), (5, 1), (5, 4), (5, 8), (5, 10), (5, 11), (6, 2), (6, 3), (6, 4), (6, 6), (6, 7), (6, 8), (6, 9), (6, 10), (6, 11), (7, 11), (8, 6), (8, 8), (9, 1), (10, 7), (11, 10)]\n","mean_ablate_by_lst(circuit, model, orig_score, print_output=True).item()"],"metadata":{"id":"zI9JKq-frrx3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# numwords\n","# incorr i+3\n","circuit = [(0, 1), (0, 6), (0, 7), (0, 9), (0, 10), (1, 0), (1, 5), (3, 3), (4, 4), (4, 10), (5, 4), (5, 6), (5, 8), (6, 6), (6, 10), (7, 6), (7, 10), (7, 11), (8, 8), (9, 1), (10, 7)]\n","mean_ablate_by_lst(circuit, model, orig_score, print_output=True).item()"],"metadata":{"id":"XgtbBeiarrx3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# months\n","# incorr i\n","circuit = [(0, 1), (2, 3), (2, 5), (2, 7), (2, 8), (2, 9), (4, 4), (5, 0), (5, 6), (6, 9), (6, 10), (7, 8), (7, 11), (8, 1), (8, 6), (8, 8), (8, 9), (9, 1), (9, 7), (9, 11), (10, 7), (11, 10)]\n","mean_ablate_by_lst(circuit, model, orig_score, print_output=True).item()"],"metadata":{"id":"IYcsLUjIrrx3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ablate after ipp rmv nodes"],"metadata":{"id":"GiBUeFON4dvl"}},{"cell_type":"code","source":["# digits incr\n","# incorr i+3\n","circuit = [(0, 1), (0, 2), (0, 5), (0, 7), (0, 8), (0, 10), (1, 0), (1, 1), (1, 5), (1, 7), (1, 11), (2, 0), (2, 1), (2, 2), (2, 3), (2, 6), (2, 8), (2, 9), (2, 10), (2, 11), (3, 3), (3, 4), (3, 5), (3, 7), (3, 8), (3, 9), (3, 11), (4, 4), (4, 10), (5, 1), (5, 4), (5, 6), (5, 8), (5, 11), (6, 4), (6, 6), (6, 7), (6, 8), (6, 9), (6, 10), (6, 11), (7, 11), (9, 1)]\n","circuit.remove((3, 7))\n","circuit.remove((2, 11))\n","circuit.remove((2, 8))\n","circuit.remove((3, 9))\n","circuit.remove((2, 1))\n","circuit.remove((3, 5))\n","circuit.remove((3, 11))\n","circuit.remove((1, 1))\n","circuit.remove((2, 9))\n","\n","mean_ablate_by_lst(circuit, model, orig_score, print_output=True).item()"],"metadata":{"id":"y0xuflJI4ixD"},"execution_count":null,"outputs":[]}]}